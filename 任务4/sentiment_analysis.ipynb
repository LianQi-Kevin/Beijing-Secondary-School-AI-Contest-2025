{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æƒ…ç»ªè¡¨æƒ…ç¬¦å·åˆ†ç±» - PyTorchç‰ˆæœ¬\n",
    "\n",
    "## é¡¹ç›®æ¦‚è¿°\n",
    "\n",
    "æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼Œèƒ½å¤Ÿå°†è¾“å…¥çš„æ–‡æœ¬å¥å­åˆ†ç±»ä¸ºå¯¹åº”çš„è¡¨æƒ…ç¬¦å·ã€‚ä¾‹å¦‚ï¼š\n",
    "- \"I love you\" â†’ â¤ï¸\n",
    "- \"I am happy\" â†’ ğŸ˜Š\n",
    "- \"I am hungry\" â†’ ğŸ´\n",
    "- \"We have a lot trouble\" â†’ ğŸ˜\n",
    "- \"Let us play ball\" â†’ âš¾\n",
    "\n",
    "### æŠ€æœ¯æ ˆ\n",
    "- **PyTorch**: æ·±åº¦å­¦ä¹ æ¡†æ¶\n",
    "- **LSTM**: é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼Œç”¨äºå¤„ç†åºåˆ—æ•°æ®\n",
    "- **GloVe**: é¢„è®­ç»ƒè¯å‘é‡\n",
    "- **Embedding Layer**: è¯åµŒå…¥å±‚\n",
    "\n",
    "### æˆ‘ä»¬è¦åšä»€ä¹ˆï¼Ÿ\n",
    "æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªLSTMç¥ç»ç½‘ç»œï¼Œå­¦ä¹ æ ¹æ®è¾“å…¥çš„è‹±æ–‡å¥å­ï¼ˆ10ä¸ªå•è¯ä»¥å†…ï¼‰ï¼Œåˆ†æå‡ºå¥å­çš„æƒ…æ„Ÿã€‚\n",
    "\n",
    "\n",
    "### å…³é”®æ­¥éª¤æ¦‚è¿°ï¼š\n",
    "1. ä½¿ç”¨é¢„è®­ç»ƒè¯å‘é‡\n",
    "2. æ„å»ºLSTMç¥ç»ç½‘ç»œ\n",
    "3. æ–‡æœ¬æ•°æ®çš„é¢„å¤„ç†æ–¹æ³•\n",
    "4. æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°çš„å…¨æµç¨‹\n",
    "\n",
    "\n",
    "## <font color='red'><b>ğŸ” è°ƒè¯•ä»»åŠ¡è¯´æ˜ï¼ˆå¯æŒ‰å¦‚ä¸‹æç¤ºä¿®æ”¹ç¬¬3æ­¥ï¼Œå…¶å®ƒæ­¥éª¤çš„ä»£ç ä¿æŒä¸å˜ï¼‰</b></font>\n",
    "\n",
    "<font color='red'><b>ä»»åŠ¡ç›®æ ‡ï¼šè¿™ä¸ªé¡¹ç›®éœ€è¦å®Œæˆä»¥ä¸‹æ ¸å¿ƒæ­¥éª¤ï¼š</b></font>\n",
    "<br>\n",
    "<font color='red'><b>1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†ï¼šåŠ è½½MNISTæ•°æ®é›†å¹¶è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†</b></font>\n",
    "<br>\n",
    "<font color='red'><b>2. æ¨¡å‹æ„å»ºï¼šä¿®æ”¹ç¬¬3æ­¥çš„ç¥ç»ç½‘ç»œæ¨¡å‹ç»“æ„ï¼Œè®¾è®¡åˆé€‚çš„LSTMç½‘ç»œæ¶æ„</b></font>\n",
    "<br>\n",
    "<font color='red'><b>3. è®­ç»ƒä¼˜åŒ–ï¼šè®­ç»ƒæ¨¡å‹</b></font>\n",
    "<br>\n",
    "<font color='red'><b>4. è¯„ä¼°æµ‹è¯•ï¼šåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œæµ‹è¯•é›†å‡†ç¡®ç‡è¾¾åˆ°85%ä»¥ä¸Š</b></font>\n",
    "\n",
    "è¯„åˆ†è§„åˆ™è¯´æ˜ï¼šæµ‹è¯•é›†å‡†ç¡®ç‡åœ¨\n",
    "\n",
    "81%-82%ä¹‹é—´ï¼Œç»™5åˆ†ï¼›\n",
    "<br>\n",
    "82%-83%ä¹‹é—´ï¼Œç»™10åˆ†ï¼›\n",
    "<br>\n",
    "83%-84%ä¹‹é—´ï¼Œç»™15åˆ†ï¼›\n",
    "<br>\n",
    "84%-85%ä¹‹é—´ï¼Œç»™20åˆ†ï¼›\n",
    "<br>\n",
    "è¶…è¿‡85%ï¼Œç»™25åˆ†ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ç›®å½•\n",
    "1. [ç¯å¢ƒè®¾ç½®å’Œåº“å¯¼å…¥](#1-ç¯å¢ƒè®¾ç½®å’Œåº“å¯¼å…¥)\n",
    "2. [æ•°æ®å·¥å…·å‡½æ•°](#2-æ•°æ®å·¥å…·å‡½æ•°)\n",
    "3. [PyTorchæ¨¡å‹å®šä¹‰](#3-PyTorchæ¨¡å‹å®šä¹‰)\n",
    "4. [æ•°æ®åŠ è½½å’Œé¢„å¤„ç†](#4-æ•°æ®åŠ è½½å’Œé¢„å¤„ç†)\n",
    "5. [åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹](#5-åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹)\n",
    "6. [æ¨¡å‹è¯„ä¼°](#6-æ¨¡å‹è¯„ä¼°)\n",
    "7. [æµ‹è¯•è‡ªå®šä¹‰å¥å­](#7-æµ‹è¯•è‡ªå®šä¹‰å¥å­)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1-ç¯å¢ƒè®¾ç½®å’Œåº“å¯¼å…¥\"></a>\n",
    "## 1. ç¯å¢ƒè®¾ç½®å’Œåº“å¯¼å…¥\n",
    "\n",
    "åœ¨è¿™ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬å°†å¯¼å…¥æ‰€æœ‰å¿…è¦çš„åº“å¹¶è¿›è¡ŒåŸºç¡€è®¾ç½®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import emoji\n",
    "import csv\n",
    "import warnings\n",
    "\n",
    "# å¿½ç•¥è­¦å‘Šä¿¡æ¯ï¼Œä½¿è¾“å‡ºæ›´æ¸…æ™°\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º - è§£å†³ä¸­æ–‡ä¹±ç é—®é¢˜\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾\n",
    "plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·\n",
    "\n",
    "# è®¾ç½®å›¾åƒå¤§å°å’Œåˆ†è¾¨ç‡\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦æœ‰GPUå¯ç”¨ï¼Œå¦‚æœæœ‰åˆ™ä½¿ç”¨GPUåŠ é€Ÿè®­ç»ƒ\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"å½“å‰ä½¿ç”¨çš„è®¾å¤‡: {device}\")\n",
    "print(\"å¦‚æœæ˜¾ç¤º'cuda'è¡¨ç¤ºä½¿ç”¨GPUåŠ é€Ÿï¼Œæ˜¾ç¤º'cpu'è¡¨ç¤ºä½¿ç”¨CPU\")\n",
    "\n",
    "# å¦‚æœä½¿ç”¨GPUï¼Œæ˜¾ç¤ºGPUä¿¡æ¯\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPUå‹å·: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2-æ•°æ®å·¥å…·å‡½æ•°\"></a>\n",
    "## 2. æ•°æ®å·¥å…·å‡½æ•°\n",
    "\n",
    "è¿™é‡Œæˆ‘ä»¬å®šä¹‰æ‰€æœ‰æ•°æ®å¤„ç†å’Œå·¥å…·å‡½æ•°ã€‚è¿™äº›å‡½æ•°è´Ÿè´£ï¼š\n",
    "- è¯»å–æ•°æ®æ–‡ä»¶\n",
    "- é¢„å¤„ç†æ–‡æœ¬\n",
    "- è½¬æ¢æ•°æ®æ ¼å¼\n",
    "- å¯è§†åŒ–ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    \"\"\"\n",
    "    è¯»å–GloVeé¢„è®­ç»ƒè¯å‘é‡æ–‡ä»¶\n",
    "    \n",
    "    å‚æ•°:\n",
    "        glove_file: GloVeè¯å‘é‡æ–‡ä»¶è·¯å¾„\n",
    "    \n",
    "    è¿”å›:\n",
    "        words_to_index: è¯åˆ°ç´¢å¼•çš„æ˜ å°„å­—å…¸\n",
    "        index_to_words: ç´¢å¼•åˆ°è¯çš„æ˜ å°„å­—å…¸  \n",
    "        word_to_vec_map: è¯åˆ°å‘é‡çš„æ˜ å°„å­—å…¸\n",
    "    \"\"\"\n",
    "    print(f\"æ­£åœ¨è¯»å–GloVeè¯å‘é‡æ–‡ä»¶: {glove_file}\")\n",
    "    \n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        \n",
    "        # é€è¡Œè¯»å–æ–‡ä»¶\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "\n",
    "        # åˆ›å»ºè¯æ±‡è¡¨ç´¢å¼•\n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        \n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "            \n",
    "    print(f\"æˆåŠŸåŠ è½½ {len(words_to_index)} ä¸ªå•è¯çš„è¯å‘é‡\")\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "def read_csv(filename='train_emoji.csv'):\n",
    "    \"\"\"\n",
    "    è¯»å–åŒ…å«æ–‡æœ¬å’Œè¡¨æƒ…æ ‡ç­¾çš„CSVæ–‡ä»¶\n",
    "    \n",
    "    å‚æ•°:\n",
    "        filename: CSVæ–‡ä»¶è·¯å¾„\n",
    "    \n",
    "    è¿”å›:\n",
    "        X: æ–‡æœ¬å¥å­æ•°ç»„\n",
    "        Y: è¡¨æƒ…æ ‡ç­¾æ•°ç»„\n",
    "    \"\"\"\n",
    "    phrase = []\n",
    "    emoji_list = []\n",
    "\n",
    "    with open(filename) as csvDataFile:\n",
    "        csvReader = csv.reader(csvDataFile)\n",
    "\n",
    "        for row in csvReader:\n",
    "            phrase.append(row[0])\n",
    "            emoji_list.append(row[1])\n",
    "\n",
    "    X = np.asarray(phrase)\n",
    "    Y = np.asarray(emoji_list, dtype=int)\n",
    "\n",
    "    print(f\"ä» {filename} åŠ è½½äº† {len(X)} ä¸ªæ ·æœ¬\")\n",
    "    return X, Y\n",
    "\n",
    "def convert_to_one_hot(Y, C):\n",
    "    \"\"\"\n",
    "    å°†æ ‡ç­¾è½¬æ¢ä¸ºone-hotç¼–ç æ ¼å¼\n",
    "    \n",
    "    å‚æ•°:\n",
    "        Y: åŸå§‹æ ‡ç­¾æ•°ç»„\n",
    "        C: ç±»åˆ«æ•°é‡\n",
    "    \n",
    "    è¿”å›:\n",
    "        one-hotç¼–ç åçš„æ ‡ç­¾æ•°ç»„\n",
    "    \"\"\"\n",
    "    Y_one_hot = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y_one_hot\n",
    "\n",
    "# å®šä¹‰è¡¨æƒ…ç¬¦å·æ˜ å°„å­—å…¸\n",
    "emoji_dictionary = {\n",
    "    \"0\": \"â¤ï¸\",\n",
    "    \"1\": \"âš¾\",\n",
    "    \"2\": \"ğŸ˜Š\",\n",
    "    \"3\": \"ğŸ˜\",\n",
    "    \"4\": \"ğŸ´\"\n",
    "}\n",
    "\n",
    "def label_to_emoji(label):\n",
    "    \"\"\"\n",
    "    å°†æ•°å­—æ ‡ç­¾è½¬æ¢ä¸ºå¯¹åº”çš„è¡¨æƒ…ç¬¦å·\n",
    "    \n",
    "    å‚æ•°:\n",
    "        label: æ•°å­—æ ‡ç­¾ (0-4)\n",
    "    \n",
    "    è¿”å›:\n",
    "        å¯¹åº”çš„è¡¨æƒ…ç¬¦å·å­—ç¬¦ä¸²\n",
    "    \"\"\"\n",
    "    label_str = str(label)\n",
    "    if label_str in emoji_dictionary:\n",
    "        return emoji_dictionary[label_str]\n",
    "    else:\n",
    "        return \"â“\"\n",
    "\n",
    "def print_predictions(X, pred):\n",
    "    \"\"\"\n",
    "    æ‰“å°é¢„æµ‹ç»“æœï¼Œæ˜¾ç¤ºæ–‡æœ¬å’Œå¯¹åº”çš„é¢„æµ‹è¡¨æƒ…\n",
    "    \n",
    "    å‚æ•°:\n",
    "        X: åŸå§‹æ–‡æœ¬æ•°ç»„\n",
    "        pred: é¢„æµ‹ç»“æœæ•°ç»„\n",
    "    \"\"\"\n",
    "    print(\"\\né¢„æµ‹ç»“æœ:\")\n",
    "    for i in range(X.shape[0]):\n",
    "        print(f\"æ–‡æœ¬: '{X[i]}' -> é¢„æµ‹è¡¨æƒ…: {label_to_emoji(int(pred[i]))}\")\n",
    "\n",
    "def plot_confusion_matrix(y_actu, y_pred, title='æ··æ·†çŸ©é˜µ', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    ç»˜åˆ¶æ··æ·†çŸ©é˜µï¼Œå¯è§†åŒ–æ¨¡å‹é¢„æµ‹æ€§èƒ½\n",
    "    \n",
    "    å‚æ•°:\n",
    "        y_actu: çœŸå®æ ‡ç­¾\n",
    "        y_pred: é¢„æµ‹æ ‡ç­¾\n",
    "        title: å›¾è¡¨æ ‡é¢˜\n",
    "        cmap: é¢œè‰²æ˜ å°„\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_actu, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    tick_marks = np.arange(len(emoji_dictionary))\n",
    "    plt.xticks(tick_marks, [label_to_emoji(i) for i in range(len(emoji_dictionary))])\n",
    "    plt.yticks(tick_marks, [label_to_emoji(i) for i in range(len(emoji_dictionary))])\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('çœŸå®æ ‡ç­¾')\n",
    "    plt.xlabel('é¢„æµ‹æ ‡ç­¾')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    å°†æ–‡æœ¬å¥å­è½¬æ¢ä¸ºå•è¯ç´¢å¼•åºåˆ—\n",
    "    \n",
    "    å‚æ•°:\n",
    "        X: æ–‡æœ¬å¥å­æ•°ç»„\n",
    "        word_to_index: è¯åˆ°ç´¢å¼•çš„æ˜ å°„å­—å…¸\n",
    "        max_len: æœ€å¤§åºåˆ—é•¿åº¦\n",
    "    \n",
    "    è¿”å›:\n",
    "        X_indices: ç´¢å¼•åºåˆ—æ•°ç»„ï¼Œå½¢çŠ¶ä¸º (æ ·æœ¬æ•°, æœ€å¤§åºåˆ—é•¿åº¦)\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    X_indices = np.zeros((m, max_len), dtype=np.int64)\n",
    "    \n",
    "    for i in range(m):\n",
    "        sentence_words = (X[i].lower()).split()\n",
    "        j = 0\n",
    "        \n",
    "        for w in sentence_words:\n",
    "            if j < max_len:\n",
    "                if w in word_to_index:\n",
    "                    X_indices[i, j] = word_to_index[w]\n",
    "                else:\n",
    "                    X_indices[i, j] = 0\n",
    "                j += 1\n",
    "                \n",
    "    print(f\"å·²å°† {m} ä¸ªå¥å­è½¬æ¢ä¸ºç´¢å¼•åºåˆ—ï¼Œåºåˆ—é•¿åº¦: {max_len}\")\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3-PyTorchæ¨¡å‹å®šä¹‰\"></a>\n",
    "## 3. PyTorchæ¨¡å‹å®šä¹‰\n",
    "\n",
    "åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†å®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹çš„ç»“æ„ã€‚æˆ‘ä»¬çš„æ¨¡å‹åŒ…å«ï¼š\n",
    "1. **é¢„è®­ç»ƒè¯åµŒå…¥å±‚**: å°†å•è¯ç´¢å¼•è½¬æ¢ä¸ºè¯å‘é‡\n",
    "2. **LSTMå±‚**: å¤„ç†åºåˆ—æ•°æ®ï¼Œæ•æ‰ä¸Šä¸‹æ–‡ä¿¡æ¯\n",
    "3. **å…¨è¿æ¥å±‚**: è¾“å‡ºåˆ†ç±»ç»“æœ\n",
    "\n",
    "<div style=\"color: #FF0000; font-size: 18px; font-weight: bold; background-color: #FFF0F0; padding: 10px; border: 2px solid #FF0000; border-radius: 5px;\"> è€ƒè¯•ä»»åŠ¡ï¼šä¿®æ”¹ä¸‹é¢SentimentAnalysisModel(nn.Module)é‡Œçš„ç½‘ç»œæ¨¡å‹é…ç½®ï¼Œä½¿å…¶åœ¨ç¬¬6æ­¥çš„æ¨¡å‹è¯„ä¼°ä¸­ï¼Œæµ‹è¯•é›†å‡†ç¡®ç‡è¶…è¿‡85% </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedEmbeddingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    é¢„è®­ç»ƒè¯åµŒå…¥å±‚\n",
    "    \n",
    "    è¿™ä¸ªå±‚ä½¿ç”¨é¢„è®­ç»ƒçš„GloVeè¯å‘é‡æ¥åˆå§‹åŒ–åµŒå…¥çŸ©é˜µã€‚\n",
    "    åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¿™äº›è¯å‘é‡ä¿æŒä¸å˜ï¼ˆä¸æ›´æ–°ï¼‰ã€‚\n",
    "    \"\"\"\n",
    "    def __init__(self, word_to_vec_map, word_to_index, emb_dim=50):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–åµŒå…¥å±‚\n",
    "        \n",
    "        å‚æ•°:\n",
    "            word_to_vec_map: è¯åˆ°å‘é‡çš„æ˜ å°„å­—å…¸\n",
    "            word_to_index: è¯åˆ°ç´¢å¼•çš„æ˜ å°„å­—å…¸\n",
    "            emb_dim: è¯å‘é‡ç»´åº¦ï¼ˆé»˜è®¤50ï¼‰\n",
    "        \"\"\"\n",
    "        super(PretrainedEmbeddingLayer, self).__init__()\n",
    "        \n",
    "        vocab_len = len(word_to_index) + 1\n",
    "        emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "        \n",
    "        for word, index in word_to_index.items():\n",
    "            emb_matrix[index, :] = word_to_vec_map[word]\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(emb_matrix), \n",
    "            freeze=True,\n",
    "            padding_idx=0\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­\n",
    "        \n",
    "        å‚æ•°:\n",
    "            x: è¾“å…¥å¼ é‡ï¼ŒåŒ…å«å•è¯ç´¢å¼•ï¼Œå½¢çŠ¶ä¸º (batch_size, sequence_length)\n",
    "        \n",
    "        è¿”å›:\n",
    "            è¯å‘é‡å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, sequence_length, embedding_dim)\n",
    "        \"\"\"\n",
    "        return self.embedding(x)\n",
    "\n",
    "class SentimentAnalysisModel(nn.Module):\n",
    "    \"\"\"\n",
    "    æƒ…æ„Ÿåˆ†ææ¨¡å‹\n",
    "    \n",
    "    è¿™ä¸ªæ¨¡å‹ä½¿ç”¨åŒå±‚LSTMæ¥å¤„ç†æ–‡æœ¬åºåˆ—ï¼Œæœ€åé€šè¿‡å…¨è¿æ¥å±‚è¾“å‡ºåˆ†ç±»ç»“æœã€‚\n",
    "    \"\"\"\n",
    "    def __init__(self, word_to_vec_map, word_to_index, hidden_dim=128, output_dim=5, dropout_rate=0.2):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–æ¨¡å‹\n",
    "        \n",
    "        å‚æ•°:\n",
    "            word_to_vec_map: è¯åˆ°å‘é‡çš„æ˜ å°„å­—å…¸\n",
    "            word_to_index: è¯åˆ°ç´¢å¼•çš„æ˜ å°„å­—å…¸\n",
    "            hidden_dim: LSTMéšè—å±‚ç»´åº¦ï¼ˆé»˜è®¤128ï¼‰\n",
    "            output_dim: è¾“å‡ºç»´åº¦ï¼Œå³ç±»åˆ«æ•°é‡ï¼ˆé»˜è®¤5ï¼‰\n",
    "            dropout_rate: Dropoutæ¯”ç‡ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆï¼ˆé»˜è®¤0.2ï¼‰\n",
    "        \"\"\"\n",
    "        super(SentimentAnalysisModel, self).__init__()\n",
    "        \n",
    "        self.embedding_layer = PretrainedEmbeddingLayer(word_to_vec_map, word_to_index)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(50, hidden_dim, batch_first=True, bidirectional=False)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, bidirectional=False)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.lstm3 = nn.LSTM(hidden_dim, hidden_dim//2, batch_first=True, bidirectional=False)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.lstm4 = nn.LSTM(hidden_dim//2, hidden_dim//4, batch_first=True, bidirectional=False)\n",
    "        self.dropout4 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim//4, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­è¿‡ç¨‹\n",
    "        \n",
    "        å‚æ•°:\n",
    "            x: è¾“å…¥å¼ é‡ï¼ŒåŒ…å«å•è¯ç´¢å¼•ï¼Œå½¢çŠ¶ä¸º (batch_size, sequence_length)\n",
    "        \n",
    "        è¿”å›:\n",
    "            output: åˆ†ç±»æ¦‚ç‡ï¼Œå½¢çŠ¶ä¸º (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        embeddings = self.embedding_layer(x)\n",
    "        \n",
    "        lstm1_out, (h_n1, c_n1) = self.lstm1(embeddings)\n",
    "        lstm1_out = self.dropout1(lstm1_out)\n",
    "        \n",
    "        lstm2_out, (h_n2, c_n2) = self.lstm2(lstm1_out)\n",
    "        lstm2_out = self.dropout2(lstm2_out)\n",
    "\n",
    "        lstm3_out, (h_n3, c_n3) = self.lstm3(lstm2_out)\n",
    "        lstm3_out = self.dropout3(lstm3_out)\n",
    "\n",
    "        lstm4_out, (h_n4, c_n4) = self.lstm4(lstm3_out)\n",
    "        lstm4_out = self.dropout4(h_n4[-1])\n",
    "        \n",
    "        output = self.fc(lstm4_out)\n",
    "        output = self.softmax(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"æ¨¡å‹ç±»å®šä¹‰å®Œæˆï¼\")\n",
    "print(\"æˆ‘ä»¬å®šä¹‰äº†ä¸¤ä¸ªç±»:\")\n",
    "print(\"1. PretrainedEmbeddingLayer: å¤„ç†è¯å‘é‡åµŒå…¥\")\n",
    "print(\"2. SentimentAnalysisModel: ä¸»è¦çš„æƒ…æ„Ÿåˆ†ææ¨¡å‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4-æ•°æ®åŠ è½½å’Œé¢„å¤„ç†\"></a>\n",
    "## 4. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†\n",
    "\n",
    "ç°åœ¨æˆ‘ä»¬å°†åŠ è½½æ•°æ®å¹¶è¿›è¡Œé¢„å¤„ç†ï¼Œä¸ºæ¨¡å‹è®­ç»ƒåšå‡†å¤‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¯»å–æ•°æ®\n",
    "print(\"è¯»å–æ•°æ®æ–‡ä»¶...\")\n",
    "try:\n",
    "    X_train, Y_train = read_csv('train_emoji.csv')\n",
    "    X_test, Y_test = read_csv('test_emoji.csv')\n",
    "    print(\"âœ“ æˆåŠŸåŠ è½½æ•°æ®æ–‡ä»¶\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âš  è­¦å‘Š: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {e}\")\n",
    "    print(\"\\nè¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶åœ¨å½“å‰ç›®å½•:\")\n",
    "    print(\"- train_emoji.csv - è®­ç»ƒæ•°æ®\")\n",
    "    print(\"- test_emoji.csv  - æµ‹è¯•æ•°æ®\") \n",
    "    print(\"- glove.6B.50d.txt - GloVeè¯å‘é‡\")\n",
    "    \n",
    "    print(\"\\næ­£åœ¨åˆ›å»ºç¤ºä¾‹æ•°æ®ç”¨äºæ¼”ç¤º...\")\n",
    "    \n",
    "    X_train = np.array([\n",
    "        'I love you',\n",
    "        'I am happy',\n",
    "        'I am sad',\n",
    "        'let us play',\n",
    "        'I am hungry',\n",
    "        'you are my love',\n",
    "        'this is fun',\n",
    "        'I feel bad',\n",
    "        'play baseball',\n",
    "        'I want food'\n",
    "    ])\n",
    "    Y_train = np.array([0, 2, 3, 1, 4, 0, 2, 3, 1, 4])\n",
    "    \n",
    "    X_test = np.array([\n",
    "        'I like you',\n",
    "        'I feel good',\n",
    "        'I am disappointed',\n",
    "        'play ball',\n",
    "        'food time'\n",
    "    ])\n",
    "    Y_test = np.array([0, 2, 3, 1, 4])\n",
    "    \n",
    "    print(\"âœ“ ç¤ºä¾‹æ•°æ®åˆ›å»ºå®Œæˆ\")\n",
    "\n",
    "# è®¡ç®—æœ€å¤§åºåˆ—é•¿åº¦\n",
    "print(\"\\nè®¡ç®—åºåˆ—é•¿åº¦...\")\n",
    "maxLen = len(max(X_train, key=len).split())\n",
    "print(f\"è®­ç»ƒé›†å¤§å°: {X_train.shape[0]} ä¸ªæ ·æœ¬\")\n",
    "print(f\"æµ‹è¯•é›†å¤§å°: {X_test.shape[0]} ä¸ªæ ·æœ¬\")\n",
    "print(f\"æœ€å¤§åºåˆ—é•¿åº¦: {maxLen} ä¸ªå•è¯\")\n",
    "\n",
    "# æ˜¾ç¤ºä¸€äº›æ ·æœ¬\n",
    "print(\"\\næ˜¾ç¤ºæ•°æ®æ ·æœ¬...\")\n",
    "print(\"è®­ç»ƒé›†å‰5ä¸ªæ ·æœ¬:\")\n",
    "for i in range(min(5, len(X_train))):\n",
    "    print(f\"  æ–‡æœ¬: '{X_train[i]:15}' -> æ ‡ç­¾: {Y_train[i]} {label_to_emoji(Y_train[i])}\")\n",
    "\n",
    "# è½¬æ¢ä¸ºone-hotç¼–ç \n",
    "print(\"\\næ•°æ®ç¼–ç ...\")\n",
    "Y_oh_train = convert_to_one_hot(Y_train, C=5)\n",
    "Y_oh_test = convert_to_one_hot(Y_test, C=5)\n",
    "print(f\"one-hotç¼–ç å½¢çŠ¶: {Y_oh_train.shape}\")\n",
    "\n",
    "# è¯»å–GloVeè¯å‘é‡\n",
    "print(\"\\nåŠ è½½é¢„è®­ç»ƒè¯å‘é‡...\")\n",
    "try:\n",
    "    word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')\n",
    "    print(f\"âœ“ æˆåŠŸåŠ è½½GloVeè¯å‘é‡\")\n",
    "    print(f\"è¯æ±‡è¡¨å¤§å°: {len(word_to_index)} ä¸ªå•è¯\")\n",
    "    print(f\"è¯å‘é‡ç»´åº¦: {word_to_vec_map['the'].shape[0]} ç»´\")\n",
    "    \n",
    "    print(\"\\nç¤ºä¾‹å•è¯çš„è¯å‘é‡:\")\n",
    "    sample_words = ['love', 'happy', 'sad', 'play', 'food']\n",
    "    for word in sample_words:\n",
    "        if word in word_to_vec_map:\n",
    "            print(f\"  '{word}': å‘é‡å½¢çŠ¶ {word_to_vec_map[word].shape}\")\n",
    "            \n",
    "except FileNotFoundError:\n",
    "    print(\"âš  è­¦å‘Š: æ‰¾ä¸åˆ°GloVeæ–‡ä»¶ï¼Œåˆ›å»ºå°å‹è¯æ±‡è¡¨ç”¨äºæ¼”ç¤º\")\n",
    "    \n",
    "    word_to_index = {\n",
    "        'i': 1, 'love': 2, 'you': 3, 'am': 4, 'happy': 5, 'sad': 6, \n",
    "        'let': 7, 'us': 8, 'play': 9, 'hungry': 10, 'like': 11, \n",
    "        'feel': 12, 'good': 13, 'disappointed': 14, 'ball': 15, \n",
    "        'food': 16, 'time': 17, 'are': 18, 'my': 19, 'this': 20, \n",
    "        'is': 21, 'fun': 22, 'bad': 23, 'baseball': 24, 'want': 25\n",
    "    }\n",
    "    \n",
    "    word_to_vec_map = {word: np.random.randn(50) for word in word_to_index.keys()}\n",
    "    \n",
    "    print(\"âœ“ æ¼”ç¤ºè¯æ±‡è¡¨åˆ›å»ºå®Œæˆ\")\n",
    "    print(f\"è¯æ±‡è¡¨å¤§å°: {len(word_to_index)} ä¸ªå•è¯\")\n",
    "\n",
    "# å‡†å¤‡æ•°æ®\n",
    "print(\"\\nå‡†å¤‡è®­ç»ƒæ•°æ®...\")\n",
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
    "\n",
    "# è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶ç§»åŠ¨åˆ°ç›¸åº”è®¾å¤‡\n",
    "print(\"\\n è½¬æ¢ä¸ºPyTorchå¼ é‡...\")\n",
    "X_train_tensor = torch.LongTensor(X_train_indices).to(device)\n",
    "Y_train_tensor = torch.FloatTensor(Y_oh_train).to(device)\n",
    "X_test_tensor = torch.LongTensor(X_test_indices).to(device)\n",
    "Y_test_tensor = torch.FloatTensor(Y_oh_test).to(device)\n",
    "\n",
    "print(f\"è®­ç»ƒæ•°æ®å½¢çŠ¶: {X_train_tensor.shape}\")\n",
    "print(f\"è®­ç»ƒæ ‡ç­¾å½¢çŠ¶: {Y_train_tensor.shape}\")\n",
    "print(f\"æµ‹è¯•æ•°æ®å½¢çŠ¶: {X_test_tensor.shape}\")\n",
    "print(f\"æµ‹è¯•æ ‡ç­¾å½¢çŠ¶: {Y_test_tensor.shape}\")\n",
    "\n",
    "print(\"\\nğŸ‰ æ•°æ®é¢„å¤„ç†å®Œæˆï¼æ‰€æœ‰æ•°æ®å·²å‡†å¤‡å¥½ç”¨äºè®­ç»ƒã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5-åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹\"></a>\n",
    "## 5. åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹\n",
    "\n",
    "ç°åœ¨æˆ‘ä»¬å°†åˆ›å»ºæ¨¡å‹å®ä¾‹å¹¶å¼€å§‹è®­ç»ƒè¿‡ç¨‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºæ¨¡å‹å®ä¾‹\n",
    "print(\" åˆ›å»ºæ¨¡å‹å®ä¾‹...\")\n",
    "model = SentimentAnalysisModel(word_to_vec_map, word_to_index).to(device)\n",
    "print(\"âœ“ æ¨¡å‹åˆ›å»ºå®Œæˆ\")\n",
    "\n",
    "# æ‰“å°æ¨¡å‹ç»“æ„\n",
    "print(\"\\næ˜¾ç¤ºæ¨¡å‹ç»“æ„...\")\n",
    "print(\"æ¨¡å‹ç»“æ„:\")\n",
    "print(model)\n",
    "\n",
    "# è®¡ç®—å‚æ•°æ•°é‡\n",
    "print(\"\\nè®¡ç®—æ¨¡å‹å‚æ•°...\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"æ€»å‚æ•°æ•°é‡: {total_params:,}\")\n",
    "print(f\"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}\")\n",
    "print(f\"å†»ç»“å‚æ•°æ•°é‡: {total_params - trainable_params:,} (è¯åµŒå…¥å±‚)\")\n",
    "\n",
    "# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨\n",
    "print(\"\\nå®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨...\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(f\"æŸå¤±å‡½æ•°: {criterion.__class__.__name__}\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "print(f\"ä¼˜åŒ–å™¨: {optimizer.__class__.__name__}\")\n",
    "print(f\"å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "print(\"\\nå¼€å§‹è®­ç»ƒæ¨¡å‹...\")\n",
    "epochs = 100\n",
    "batch_size = 8\n",
    "\n",
    "print(f\"è®­ç»ƒé…ç½®:\")\n",
    "print(f\"- è®­ç»ƒè½®æ•°: {epochs}\")\n",
    "print(f\"- æ‰¹å¤§å°: {batch_size}\")\n",
    "print(f\"- è®­ç»ƒæ ·æœ¬æ•°: {len(X_train_tensor)}\")\n",
    "print(f\"- æ¯è½®æ‰¹æ¬¡æ•°: {len(X_train_tensor) // batch_size + 1}\")\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "print(\"\\nå¼€å§‹è®­ç»ƒå¾ªç¯...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    permutation = torch.randperm(X_train_tensor.size(0))\n",
    "    \n",
    "    for i in range(0, X_train_tensor.size(0), batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x = X_train_tensor[indices]\n",
    "        batch_y = Y_train_tensor[indices]\n",
    "        \n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(X_train_tensor)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f'è½®æ¬¡ [{epoch+1:3d}/{epochs}], å¹³å‡æŸå¤±: {avg_loss:.4f}')\n",
    "\n",
    "print(\"\\nğŸ‰ æ¨¡å‹è®­ç»ƒå®Œæˆï¼\")\n",
    "\n",
    "# ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿\n",
    "print(\"\\nç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿...\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, linewidth=2)\n",
    "plt.title('è®­ç»ƒæŸå¤±å˜åŒ–æ›²çº¿', fontsize=14)\n",
    "plt.xlabel('è®­ç»ƒè½®æ¬¡', fontsize=12)\n",
    "plt.ylabel('æŸå¤±å€¼', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.text(len(train_losses)*0.7, max(train_losses)*0.8, \n",
    "         f'æœ€ç»ˆæŸå¤±: {train_losses[-1]:.4f}', \n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š æŸå¤±æ›²çº¿è¯´æ˜:\")\n",
    "print(\"- æ›²çº¿ä¸‹é™è¡¨ç¤ºæ¨¡å‹æ­£åœ¨å­¦ä¹ \")\n",
    "print(\"- æ›²çº¿å¹³ç¨³è¡¨ç¤ºæ¨¡å‹å¯èƒ½å·²ç»æ”¶æ•›\")\n",
    "print(\"- å¦‚æœæ›²çº¿æ³¢åŠ¨å¾ˆå¤§ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´å­¦ä¹ ç‡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6-æ¨¡å‹è¯„ä¼°\"></a>\n",
    "## 6. æ¨¡å‹è¯„ä¼°\n",
    "\n",
    "ç°åœ¨æˆ‘ä»¬å°†è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šçš„è¡¨ç°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¯„ä¼°æ¨¡å‹\n",
    "print(\"è¯„ä¼°æ¨¡å‹æ€§èƒ½...\")\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"\\nè®¡ç®—è®­ç»ƒé›†å‡†ç¡®ç‡...\")\n",
    "    train_outputs = model(X_train_tensor)\n",
    "    train_pred = torch.argmax(train_outputs, dim=1)\n",
    "    train_true = torch.argmax(Y_train_tensor, dim=1)\n",
    "    train_accuracy = (train_pred == train_true).float().mean()\n",
    "    \n",
    "    print(f\"è®­ç»ƒé›†å‡†ç¡®ç‡: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "    \n",
    "    print(\"\\nè®¡ç®—æµ‹è¯•é›†å‡†ç¡®ç‡...\")\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    test_pred = torch.argmax(test_outputs, dim=1)\n",
    "    test_true = torch.argmax(Y_test_tensor, dim=1)\n",
    "    test_accuracy = (test_pred == test_true).float().mean()\n",
    "    \n",
    "    print(f\"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "    \n",
    "    test_loss = criterion(test_outputs, Y_test_tensor)\n",
    "    print(f\"æµ‹è¯•é›†æŸå¤±: {test_loss:.4f}\")\n",
    "\n",
    "# æ˜¾ç¤ºé”™è¯¯é¢„æµ‹çš„æ ·æœ¬\n",
    "print(\"\\nåˆ†æé”™è¯¯é¢„æµ‹...\")\n",
    "C = 5\n",
    "y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
    "pred_np = test_outputs.cpu().numpy()\n",
    "\n",
    "error_count = 0\n",
    "error_samples = []\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    num = np.argmax(pred_np[i])\n",
    "    if (num != Y_test[i]):\n",
    "        error_count += 1\n",
    "        error_samples.append({\n",
    "            'text': X_test[i],\n",
    "            'true_label': Y_test[i],\n",
    "            'pred_label': num,\n",
    "            'true_emoji': label_to_emoji(Y_test[i]),\n",
    "            'pred_emoji': label_to_emoji(num)\n",
    "        })\n",
    "\n",
    "print(f\"æ€»å…±é”™è¯¯é¢„æµ‹: {error_count}/{len(X_test)}\")\n",
    "\n",
    "if error_count > 0:\n",
    "    print(\"\\né”™è¯¯é¢„æµ‹æ ·æœ¬è¯¦æƒ…:\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, error in enumerate(error_samples[:10]):\n",
    "        print(f\"æ ·æœ¬ {i+1}:\")\n",
    "        print(f\"  æ–‡æœ¬: '{error['text']}'\")\n",
    "        print(f\"  çœŸå®: {error['true_emoji']} (æ ‡ç­¾{error['true_label']})\")\n",
    "        print(f\"  é¢„æµ‹: {error['pred_emoji']} (æ ‡ç­¾{error['pred_label']})\")\n",
    "        print(\"-\" * 60)\n",
    "else:\n",
    "    print(\"ğŸ‰ å®Œç¾ï¼æ‰€æœ‰æµ‹è¯•æ ·æœ¬éƒ½é¢„æµ‹æ­£ç¡®ï¼\")\n",
    "\n",
    "# ç»˜åˆ¶æ··æ·†çŸ©é˜µ\n",
    "print(\"\\nç»˜åˆ¶æ··æ·†çŸ©é˜µ...\")\n",
    "y_true = test_true.cpu().numpy()\n",
    "y_pred = test_pred.cpu().numpy()\n",
    "\n",
    "print(\"æ··æ·†çŸ©é˜µè¯´æ˜:\")\n",
    "print(\"- å¯¹è§’çº¿ä¸Šçš„æ•°å­—è¡¨ç¤ºæ­£ç¡®é¢„æµ‹çš„æ•°é‡\")\n",
    "print(\"- éå¯¹è§’çº¿ä¸Šçš„æ•°å­—è¡¨ç¤ºé”™è¯¯é¢„æµ‹çš„æ•°é‡\")\n",
    "print(\"- è¡Œè¡¨ç¤ºçœŸå®æ ‡ç­¾ï¼Œåˆ—è¡¨ç¤ºé¢„æµ‹æ ‡ç­¾\")\n",
    "\n",
    "plot_confusion_matrix(y_true, y_pred, 'æ¨¡å‹æ··æ·†çŸ©é˜µ')\n",
    "\n",
    "# æ¨¡å‹æ€§èƒ½æ€»ç»“\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ“ˆ æ¨¡å‹æ€§èƒ½æ€»ç»“\")\n",
    "print(\"=\"*50)\n",
    "print(f\"è®­ç»ƒé›†å‡†ç¡®ç‡: {train_accuracy*100:.2f}%\")\n",
    "print(f\"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy*100:.2f}%\")\n",
    "print(f\"æµ‹è¯•é›†æŸå¤±: {test_loss:.4f}\")\n",
    "print(f\"é”™è¯¯é¢„æµ‹æ•°: {error_count}/{len(X_test)}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if test_accuracy > 0.85:\n",
    "    print(\"ğŸ‰ ä¼˜ç§€ï¼æ¨¡å‹è¡¨ç°éå¸¸å¥½ï¼\")\n",
    "elif test_accuracy > 0.8:\n",
    "    print(\"ğŸ‘ è‰¯å¥½ï¼æ¨¡å‹è¡¨ç°ä¸é”™ï¼\")\n",
    "elif test_accuracy > 0.5:\n",
    "    print(\"âš  ä¸€èˆ¬ï¼æ¨¡å‹è¿˜æœ‰æ”¹è¿›ç©ºé—´ã€‚\")\n",
    "else:\n",
    "    print(\"âŒ è¾ƒå·®ï¼å»ºè®®æ£€æŸ¥æ•°æ®æˆ–è°ƒæ•´æ¨¡å‹ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7-æµ‹è¯•è‡ªå®šä¹‰å¥å­\"></a>\n",
    "## 7. æµ‹è¯•è‡ªå®šä¹‰å¥å­\n",
    "\n",
    "ç°åœ¨è®©æˆ‘ä»¬ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹æ¥é¢„æµ‹ä¸€äº›è‡ªå®šä¹‰çš„å¥å­ï¼Œçœ‹çœ‹æ¨¡å‹çš„å®é™…è¡¨ç°ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•è‡ªå®šä¹‰å¥å­\n",
    "print(\"ğŸ”® ç°åœ¨è®©æˆ‘ä»¬æµ‹è¯•ä¸€äº›è‡ªå®šä¹‰å¥å­ï¼\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_sentences = [\n",
    "    'very happy',\n",
    "    'I love you',\n",
    "    'I am sad',\n",
    "    'let us play ball',\n",
    "    'I am hungry',\n",
    "    'this is amazing',\n",
    "    'I feel terrible',\n",
    "    'we won the game',\n",
    "    'time for dinner',\n",
    "    'you are my everything'\n",
    "]\n",
    "\n",
    "print(\"æµ‹è¯•å¥å­åˆ—è¡¨:\")\n",
    "for i, sentence in enumerate(test_sentences, 1):\n",
    "    print(f\"{i:2d}. '{sentence}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"é¢„æµ‹ç»“æœ:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sentence in test_sentences:\n",
    "        x_test = np.array([sentence])\n",
    "        X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "        X_test_tensor = torch.LongTensor(X_test_indices).to(device)\n",
    "        \n",
    "        prediction = model(X_test_tensor)\n",
    "        pred_class = torch.argmax(prediction, dim=1).cpu().numpy()[0]\n",
    "        pred_emoji = label_to_emoji(pred_class)\n",
    "        confidence = torch.max(prediction).cpu().numpy()\n",
    "        \n",
    "        print(f\"æ–‡æœ¬: '{sentence:20}' -> é¢„æµ‹: {pred_emoji:3} | ç½®ä¿¡åº¦: {confidence:.3f} | æ ‡ç­¾: {pred_class}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# è®©ç”¨æˆ·è¾“å…¥è‡ªå®šä¹‰å¥å­\n",
    "print(\"\\nğŸ’¬ ç°åœ¨æ‚¨å¯ä»¥æµ‹è¯•è‡ªå·±çš„å¥å­ï¼\")\n",
    "print(\"è¾“å…¥ 'quit' é€€å‡ºæµ‹è¯•\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"è¯·è¾“å…¥ä¸€ä¸ªå¥å­: \").strip()\n",
    "    \n",
    "    if user_input.lower() == 'quit':\n",
    "        print(\"è°¢è°¢ä½¿ç”¨ï¼å†è§ï¼\")\n",
    "        break\n",
    "    \n",
    "    if not user_input:\n",
    "        print(\"è¯·è¾“å…¥æœ‰æ•ˆçš„å¥å­ï¼\")\n",
    "        continue\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x_test = np.array([user_input])\n",
    "        X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "        X_test_tensor = torch.LongTensor(X_test_indices).to(device)\n",
    "        \n",
    "        prediction = model(X_test_tensor)\n",
    "        pred_class = torch.argmax(prediction, dim=1).cpu().numpy()[0]\n",
    "        pred_emoji = label_to_emoji(pred_class)\n",
    "        confidence = torch.max(prediction).cpu().numpy()\n",
    "        \n",
    "        probabilities = prediction.cpu().numpy()[0]\n",
    "        \n",
    "        print(f\"\\nğŸ” åˆ†æç»“æœ:\")\n",
    "        print(f\"   è¾“å…¥: '{user_input}'\")\n",
    "        print(f\"   é¢„æµ‹: {pred_emoji} (æ ‡ç­¾ {pred_class})\")\n",
    "        print(f\"   ç½®ä¿¡åº¦: {confidence:.3f}\")\n",
    "        \n",
    "        print(f\"\\nğŸ“Š æ‰€æœ‰ç±»åˆ«æ¦‚ç‡:\")\n",
    "        for i in range(5):\n",
    "            emoji_char = label_to_emoji(i)\n",
    "            prob = probabilities[i]\n",
    "            bar = \"â–ˆ\" * int(prob * 20)\n",
    "            print(f\"   {emoji_char} {i}: {prob:.3f} {bar}\")\n",
    "    \n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
