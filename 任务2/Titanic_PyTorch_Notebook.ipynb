{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 泰坦尼克号生存预测 - PyTorch实现\n",
    "\n",
    "## 项目介绍\n",
    "\n",
    "在这个考试任务中，我们将一起调试一个神经网络分类模型。我们将使用泰坦尼克号乘客的生存数据集，该数据集包含了如\n",
    "- Survived: 是否幸存（0=否，1=是）← 这是我们要预测的目标！\n",
    "- Pclass: 船票等级（1=头等舱，2=二等舱，3=三等舱）\n",
    "- Sex: 性别\n",
    "- Age: 年龄\n",
    "- SibSp: 兄弟姐妹/配偶数量\n",
    "- Parch: 父母/孩子数量\n",
    "- Fare: 票价\n",
    "- Embarked: 登船港口\n",
    "  \n",
    "等特征，我们的目标是构建一个神经网络分类模型来准确预测泰坦尼克号乘客的生存情况。\n",
    "\n",
    "### 背景知识\n",
    "- **泰坦尼克号**：1912年沉没的著名邮轮\n",
    "- **机器学习**：让计算机从数据中学习规律的技术\n",
    "- **神经网络**：模仿人脑工作方式的计算模型\n",
    "\n",
    "### 我们要做什么？\n",
    "我们将分析乘客的年龄、性别、船票等级等信息，来预测他们是否在沉船事故中幸存。\n",
    "\n",
    "## <font color='red'><b>🔍 调试任务说明（只修改第10步代码，其它步骤代码保持不变）</b></font>\n",
    "\n",
    "<font color='red'><b>任务目标：这个项目中有几处代码需要调试和修复。你的任务是：</b></font>\n",
    "<br>\n",
    "<font color='red'><b>1. 逐步执行此文档里的每一步代码</b></font>\n",
    "<br>\n",
    "<font color='red'><b>2. 理解代码的含义</b></font>\n",
    "<br>\n",
    "<font color='red'><b>3. 根据第10步代码的提示，修改神经网络模型的配置参数，使其验证准确率超过84%</b></font>\n",
    "\n",
    "评分规则说明：验证准确率在\n",
    "<br>\n",
    "80%-81%之间，给5分；\n",
    "<br>\n",
    "81%-82%之间，给10分；\n",
    "<br>\n",
    "82%-83%之间，给15分；\n",
    "<br>\n",
    "83%-84%之间，给20分；\n",
    "<br>\n",
    "大于84%，给25分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一步：导入工具库并设置中文显示\n",
    "\n",
    "在开始之前，我们需要导入一些Python库，并设置中文显示，这样图表就能正确显示中文了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据处理工具 - 就像Excel，用来处理表格数据\n",
    "import pandas as pd  \n",
    "import numpy as np   \n",
    "\n",
    "# 深度学习框架 - 这是我们的大脑模型工具\n",
    "import torch         \n",
    "import torch.nn as nn  # 神经网络模块\n",
    "import torch.optim as optim  # 优化器，帮助模型学习\n",
    "from torch.utils.data import DataLoader, TensorDataset  # 数据加载工具\n",
    "\n",
    "# 机器学习工具\n",
    "from sklearn.model_selection import train_test_split  # 分割数据集\n",
    "from sklearn.preprocessing import StandardScaler  # 数据标准化\n",
    "from sklearn.metrics import classification_report  # 评估报告\n",
    "\n",
    "# 画图工具 - 用来可视化结果\n",
    "import matplotlib.pyplot as plt  \n",
    "import matplotlib\n",
    "\n",
    "# 设置中文字体显示 - 解决中文乱码问题\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']  # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号\n",
    "\n",
    "# 设置图像大小和分辨率\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# 设置随机种子 - 这样每次运行结果都一样，便于学习\n",
    "# 就像做科学实验要控制变量一样\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✅ 所有工具库导入成功！\")\n",
    "print(\"✅ 中文显示设置完成！\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二步：加载数据 - 看看我们的\"原材料\"\n",
    "\n",
    "现在我们要加载泰坦尼克号的数据集，看看里面有什么信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据文件 - 就像打开一个Excel文件\n",
    "# 注意：你需要确保文件路径正确\n",
    "df_titanic = pd.read_csv('dataset.csv') \n",
    "\n",
    "# 让我们看看数据的基本信息\n",
    "print(\"📊 数据集基本信息:\")\n",
    "print(f\"数据集形状: {df_titanic.shape} ← 这表示有{df_titanic.shape[0]}行，{df_titanic.shape[1]}列\")\n",
    "print(\"\\n🔍 前5行数据预览:\")\n",
    "\n",
    "# 显示前5行数据，了解一下数据长什么样子\n",
    "df_titanic.head()\n",
    "\n",
    "# 解释一下重要的列：\n",
    "# - Survived: 是否幸存（0=否，1=是）← 这是我们要预测的目标！\n",
    "# - Pclass: 船票等级（1=头等舱，2=二等舱，3=三等舱）\n",
    "# - Sex: 性别\n",
    "# - Age: 年龄\n",
    "# - SibSp: 兄弟姐妹/配偶数量\n",
    "# - Parch: 父母/孩子数量\n",
    "# - Fare: 票价\n",
    "# - Embarked: 登船港口"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三步：创建改进的可视化函数\n",
    "\n",
    "现在我们来创建更好的可视化函数，确保中文能正常显示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_history(train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "    \"\"\"显示训练过程的学习曲线\n",
    "    \n",
    "    参数:\n",
    "    train_losses: 训练损失列表\n",
    "    val_losses: 验证损失列表  \n",
    "    train_accuracies: 训练准确率列表\n",
    "    val_accuracies: 验证准确率列表\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    # 创建画布\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # 第一个子图：损失曲线\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, 'bo-', label='训练损失', markersize=4)\n",
    "    plt.plot(epochs, val_losses, 'ro-', label='验证损失', markersize=4)\n",
    "    plt.title('训练和验证损失')\n",
    "    plt.xlabel('训练轮次')\n",
    "    plt.ylabel('损失值')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 第二个子图：准确率曲线\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, 'bo-', label='训练准确率', markersize=4)\n",
    "    plt.plot(epochs, val_accuracies, 'ro-', label='验证准确率', markersize=4)\n",
    "    plt.title('训练和验证准确率')\n",
    "    plt.xlabel('训练轮次')\n",
    "    plt.ylabel('准确率 (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_report(y_true, y_pred):\n",
    "    \"\"\"显示分类报告\n",
    "    \n",
    "    参数:\n",
    "    y_true: 真实标签\n",
    "    y_pred: 预测概率\n",
    "    \"\"\"\n",
    "    # 将预测概率转换为类别（>0.5为1，否则为0）\n",
    "    y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    print(\"📋 分类报告:\")\n",
    "    print(classification_report(y_true, y_pred_classes, labels=[0, 1], target_names=['未幸存', '幸存']))\n",
    "    \n",
    "    # 计算总体准确率\n",
    "    accuracy = (y_pred_classes == y_true).mean()\n",
    "    print(f\"总体准确率: {accuracy * 100:.2f}%\")\n",
    "\n",
    "def plot_feature_importance(df_titanic):\n",
    "    \"\"\"绘制特征重要性图（相关性分析）\"\"\"\n",
    "    # 计算特征与生存的相关性\n",
    "    numeric_df = df_titanic.select_dtypes(include=[np.number])\n",
    "    correlations = numeric_df.corr()['Survived'].drop('Survived').sort_values(ascending=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = ['red' if x < 0 else 'blue' for x in correlations]\n",
    "    plt.barh(range(len(correlations)), correlations, color=colors, alpha=0.7)\n",
    "    plt.yticks(range(len(correlations)), [\n",
    "        '性别_男', '船票等级', '登船港口_S', '兄弟姐妹', '父母孩子', \n",
    "        '登船港口_Q', '年龄', '乘客ID', '登船港口_C', '票价', '性别_女'\n",
    "    ][:len(correlations)])\n",
    "    plt.xlabel('与生存的相关性')\n",
    "    plt.title('特征与生存的相关性分析')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"✅ 可视化工具准备完成！\")\n",
    "print(\"这些工具会帮助我们理解模型的学习过程和数据分析\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四步：数据探索和可视化\n",
    "\n",
    "在开始建模之前，让我们先探索一下数据，看看各个特征与生存的关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先查看一下数据的整体信息\n",
    "print(\"🔍 数据探索:\")\n",
    "print(f\"总乘客数: {len(df_titanic)}\")\n",
    "print(f\"幸存人数: {df_titanic['Survived'].sum()}\")\n",
    "print(f\"幸存率: {df_titanic['Survived'].mean()*100:.1f}%\")\n",
    "\n",
    "# 绘制特征相关性图\n",
    "print(\"\\n📊 绘制特征重要性图...\")\n",
    "plot_feature_importance(df_titanic)\n",
    "\n",
    "print(\"\\n💡 观察：哪些特征与生存正相关（蓝色）？哪些负相关（红色）？\")\n",
    "print(\"   正相关：特征值越大，生存概率越高\")\n",
    "print(\"   负相关：特征值越大，生存概率越低\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第五步：数据预处理 - 整理和清洗数据\n",
    "\n",
    "### 5.1 看看我们要预测什么\n",
    "\n",
    "首先，我们看看有多少人幸存，多少人没有幸存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计生存情况\n",
    "print(\"🎯 生存情况统计（这是我们想要预测的目标）:\")\n",
    "survival_counts = df_titanic.Survived.value_counts()\n",
    "print(survival_counts)\n",
    "print(f\"\\n幸存率: {survival_counts[1] / len(df_titanic) * 100:.1f}%\")\n",
    "\n",
    "# 处理缺失值 - 就像补全问卷中没填的部分\n",
    "# 有些乘客的年龄信息缺失了，我们用0来填充\n",
    "df_titanic['Age'] = df_titanic['Age'].fillna(0)\n",
    "print(f\"\\n🔧 年龄缺失值处理完成！\")\n",
    "print(f\"现在年龄列的缺失值数量: {df_titanic['Age'].isnull().sum()} ← 应该是0\")\n",
    "\n",
    "# 小知识：在真实项目中，我们可能会用平均年龄来填充，但这里为了简单就用0了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 把文字变成数字 - 编码分类变量\n",
    "\n",
    "计算机不理解\"男\"、\"女\"这样的文字，只理解数字。所以我们要把文字转换成数字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔤 正在把文字特征转换为数字...\")\n",
    "\n",
    "# 对性别进行one-hot编码（独热编码）\n",
    "# 什么是one-hot编码？就是把\"男\"变成[1,0]，\"女\"变成[0,1]\n",
    "print(\"处理性别特征:\")\n",
    "sex_dummies = pd.get_dummies(df_titanic['Sex'], prefix=\"Sex\")\n",
    "print(sex_dummies.head())  # 看看转换后的样子\n",
    "\n",
    "# 对登船港口进行同样的处理\n",
    "print(\"\\n处理登船港口特征:\")\n",
    "embarked_dummies = pd.get_dummies(df_titanic['Embarked'], prefix=\"Em\")\n",
    "print(embarked_dummies.head())\n",
    "\n",
    "# 把转换后的新特征添加到原数据中\n",
    "print(\"\\n📥 将新特征合并到数据集中...\")\n",
    "df_titanic = pd.concat([df_titanic, sex_dummies, embarked_dummies], axis=1)\n",
    "\n",
    "# 删除原来的文字列（因为我们已经有了数字版本）\n",
    "df_titanic = df_titanic.drop(columns=['Sex', 'Embarked'])\n",
    "\n",
    "print(\"✅ 类别特征编码完成！\")\n",
    "print(f\"现在数据集有 {df_titanic.shape[1]} 个特征\")\n",
    "print(\"\\n新的数据集预览:\")\n",
    "df_titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第六步：准备训练数据 - 分开特征和标签\n",
    "\n",
    "现在我们要把数据分成两部分：\n",
    "- **特征（X）**: 用来预测的信息（年龄、性别等）\n",
    "- **标签（y）**: 我们要预测的目标（是否幸存）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建特征集 X - 所有用来预测的信息\n",
    "# 我们移除一些不相关或太复杂的特征\n",
    "X = df_titanic.drop(['Survived', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "\n",
    "# 构建标签集 y - 我们要预测的目标（是否幸存）\n",
    "y = df_titanic.Survived.values\n",
    "\n",
    "# 调整y的形状，让计算机更容易处理\n",
    "y = y.reshape(-1, 1)  # -1表示自动计算合适的行数\n",
    "\n",
    "print(\"🎯 数据准备完成:\")\n",
    "print(f\"特征集 X 形状: {X.shape} ← 有{X.shape[0]}个乘客，{X.shape[1]}个特征\")\n",
    "print(f\"标签集 y 形状: {y.shape} ← 有{y.shape[0]}个标签\")\n",
    "print(f\"\\n使用的特征列表: {list(X.columns)}\")\n",
    "\n",
    "# 思考：为什么我们要移除'Name', 'Ticket', 'Cabin'这些列？\n",
    "# 提示：这些信息太具体或者缺失太多，对预测帮助不大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第七步：分割数据集 - 创建练习册和考试卷\n",
    "\n",
    "我们要把数据分成两部分：\n",
    "- **训练集**: 用来教模型学习（就像练习题）\n",
    "- **测试集**: 用来检验模型学得怎么样（就像考试题）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割数据集：80%用于训练，20%用于测试\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"✂️ 数据集分割完成:\")\n",
    "print(f\"训练集 - 特征: {X_train.shape}, 标签: {y_train.shape} ← 用来教模型学习\")\n",
    "print(f\"测试集 - 特征: {X_test.shape}, 标签: {y_test.shape} ← 用来测试模型效果\")\n",
    "print(f\"\\n训练集占比: {X_train.shape[0] / len(X) * 100:.1f}%\")\n",
    "print(f\"测试集占比: {X_test.shape[0] / len(X) * 100:.1f}%\")\n",
    "\n",
    "# 小知识：为什么不能都用同样的数据来训练和测试？\n",
    "# 因为那样模型就\"作弊\"了，它已经见过所有题目的答案！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第八步：数据标准化 - 让所有特征在同一个尺度上\n",
    "\n",
    "想象一下，年龄是0-100，票价是0-500，这就像用厘米和米同时测量身高！\n",
    "我们需要把所有的特征都调整到相似的尺度上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建标准化器 - 就像一个智能尺子\n",
    "scaler = StandardScaler()\n",
    "\n",
    "print(\"📏 开始数据标准化...\")\n",
    "print(\"标准化前:\")\n",
    "print(f\"  年龄范围: {X_train['Age'].min():.1f} ~ {X_train['Age'].max():.1f}\")\n",
    "print(f\"  票价范围: {X_train['Fare'].min():.1f} ~ {X_train['Fare'].max():.1f}\")\n",
    "\n",
    "# 对训练集进行拟合和转换（计算均值和标准差，然后转换）\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# 对测试集进行转换（使用训练集的参数，避免\"偷看\"测试集）\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n标准化后:\")\n",
    "print(f\"  训练集 - 均值: {X_train_scaled.mean():.4f}, 标准差: {X_train_scaled.std():.4f}\")\n",
    "print(f\"  测试集 - 均值: {X_test_scaled.mean():.4f}, 标准差: {X_test_scaled.std():.4f}\")\n",
    "print(\"\\n✅ 数据标准化完成！现在所有特征都在相似的尺度上了\")\n",
    "\n",
    "# 标准化后的数据均值为0，标准差为1，这样模型学习起来更容易"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第九步：转换为PyTorch张量 - 准备模型能吃的\"食物\"\n",
    "\n",
    "PyTorch使用一种叫做\"张量\"的数据结构，就像NumPy数组的升级版。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔥 正在将数据转换为PyTorch张量...\")\n",
    "\n",
    "# 将数据转换为PyTorch张量\n",
    "# FloatTensor表示浮点数张量，因为我们的数据有小数\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.FloatTensor(y_test)\n",
    "\n",
    "print(\"📊 张量信息:\")\n",
    "print(f\"训练特征张量: {X_train_tensor.shape} ← 形状\")\n",
    "print(f\"训练标签张量: {y_train_tensor.shape}\")\n",
    "print(f\"测试特征张量: {X_test_tensor.shape}\")\n",
    "print(f\"测试标签张量: {y_test_tensor.shape}\")\n",
    "\n",
    "# 创建数据加载器 - 就像给模型准备小份的食物\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# batch_size=64 表示每次给模型64个样本学习\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"\\n✅ 数据加载器创建完成!\")\n",
    "print(f\"每个批次包含64个样本\")\n",
    "print(f\"总共有 {len(train_loader)} 个训练批次\")\n",
    "print(f\"总共有 {len(test_loader)} 个测试批次\")\n",
    "\n",
    "# 为什么要分批次？因为一次处理所有数据太慢了，分批次可以加快学习速度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第十步：构建神经网络模型 - 创建我们的人工\"大脑\"\n",
    "\n",
    "现在我们要构建一个神经网络，它就像一个有多个层次的大脑：\n",
    "- 输入层：接收数据\n",
    "- 隐藏层：处理信息\n",
    "- 输出层：给出答案\n",
    "\n",
    "<div style=\"color: #FF0000; font-size: 18px; font-weight: bold; background-color: #FFF0F0; padding: 10px; border: 2px solid #FF0000; border-radius: 5px;\"> 考试任务：修改下面TitanicNN(nn.Module)里self.network的网络模型配置(例如修改神经网络的层数或者增加神经元个数等)，然后执行第十一步，使其验证准确率超过84% </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义我们的神经网络类\n",
    "class TitanicNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        \"\"\"初始化神经网络\n",
    "        input_size: 输入特征的个数\n",
    "        \"\"\"\n",
    "        super(TitanicNN, self).__init__()\n",
    "        \n",
    "        # 创建神经网络层 - 就像搭建乐高积木\n",
    "        self.network = nn.Sequential(\n",
    "            # 第一层：输入层 → 隐藏层（12个神经元）\n",
    "            nn.Linear(input_size, 12),  # Linear是全连接层\n",
    "            nn.ReLU(),                  # ReLU是激活函数，让网络能学习复杂模式\n",
    "            \n",
    "            # 第二层：12个神经元 → 24个神经元\n",
    "            nn.Linear(12, 24),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 输出层：24个神经元 → 1个神经元（生存概率）\n",
    "            nn.Linear(24, 1),\n",
    "            nn.Sigmoid()  # Sigmoid把输出变成0-1之间的概率\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"定义数据如何通过网络传播\"\"\"\n",
    "        return self.network(x)\n",
    "\n",
    "# 创建模型实例\n",
    "input_size = X_train_tensor.shape[1]  # 输入特征的数量\n",
    "model = TitanicNN(input_size)\n",
    "\n",
    "# 定义损失函数 - 用来衡量模型预测的好坏\n",
    "# BCELoss = Binary Cross Entropy Loss，适合二分类问题\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 定义优化器 - 用来调整模型参数，让模型变得更好\n",
    "# RMSprop是一种智能的学习方法\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001)  # lr是学习率\n",
    "\n",
    "print(\"🧠 神经网络模型构建完成！\")\n",
    "print(f\"输入特征数量: {input_size}\")\n",
    "print(f\"学习率: 0.001\")\n",
    "print(f\"\\n模型结构:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第十一步：训练模型 - 开始教我们的\"大脑\"学习！\n",
    "\n",
    "这是最激动人心的部分！我们要开始训练模型了。\n",
    "\n",
    "训练过程就像：\n",
    "1. 给模型看一些数据（前向传播）\n",
    "2. 检查它猜得对不对（计算损失）\n",
    "3. 告诉它怎么改进（反向传播）\n",
    "4. 重复很多次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置训练轮次\n",
    "epochs = 100  # 整个数据集要学习100遍\n",
    "\n",
    "# 创建列表来记录训练过程\n",
    "train_losses = []      # 记录训练损失\n",
    "val_losses = []        # 记录验证损失\n",
    "train_accuracies = []  # 记录训练准确率\n",
    "val_accuracies = []    # 记录验证准确率\n",
    "\n",
    "print(\"🚀 开始训练模型...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # === 训练阶段 ===\n",
    "    model.train()  # 设置为训练模式\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    # 遍历每个批次\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # 前向传播：让模型做预测\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # 反向传播：计算梯度\n",
    "        optimizer.zero_grad()  # 清空之前的梯度\n",
    "        loss.backward()        # 计算新梯度\n",
    "        optimizer.step()       # 更新模型参数\n",
    "        \n",
    "        # 统计信息\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # 计算准确率\n",
    "        predicted = (outputs > 0.5).float()  # 概率>0.5预测为幸存\n",
    "        train_total += batch_y.size(0)\n",
    "        train_correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    # === 验证阶段 ===\n",
    "    model.eval()  # 设置为评估模式\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():  # 验证时不计算梯度，节省内存\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            predicted = (outputs > 0.5).float()\n",
    "            val_total += batch_y.size(0)\n",
    "            val_correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    # 计算平均损失和准确率\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    \n",
    "    # 记录结果\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    # 每5轮打印一次进度\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'第 {epoch+1:2d}/{epochs} 轮 | ' \\\n",
    "              f'训练损失: {avg_train_loss:.4f} | ' \\\n",
    "              f'验证损失: {avg_val_loss:.4f} | ' \\\n",
    "              f'训练准确率: {train_accuracy:6.2f}% | ' \\\n",
    "              f'验证准确率: {val_accuracy:6.2f}%')\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"✅ 模型训练完成！\")\n",
    "print(f\"最终验证准确率: {val_accuracies[-1]:.2f}%\")\n",
    "\n",
    "# 观察：训练准确率和验证准确率哪个更高？为什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第十二步：评估模型 - 看看我们的\"大脑\"学得怎么样\n",
    "\n",
    "现在让我们可视化训练过程，并详细评估模型性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示训练历史图表\n",
    "print(\"📈 训练过程可视化:\")\n",
    "show_history(train_losses, val_losses, train_accuracies, val_accuracies)\n",
    "\n",
    "# 在测试集上进行最终预测\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    test_predictions = test_outputs.numpy()\n",
    "\n",
    "# 显示详细的分类报告\n",
    "print(\"\\n🧪 神经网络模型详细评估:\")\n",
    "show_report(y_test, test_predictions)\n",
    "\n",
    "# 分析图表：\n",
    "# - 损失曲线应该逐渐下降\n",
    "# - 准确率曲线应该逐渐上升\n",
    "# - 如果训练准确率远高于验证准确率，可能过拟合了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第十三步：与传统方法对比 - 看看深度学习的优势\n",
    "\n",
    "让我们用传统的逻辑回归模型来对比，看看神经网络是否真的更好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"🔬 与传统机器学习方法对比...\")\n",
    "\n",
    "# 训练逻辑回归模型（传统方法）\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "lr_model.fit(X_train_scaled, y_train.ravel())  # ravel()把y从二维变一维\n",
    "\n",
    "# 计算逻辑回归的准确率\n",
    "lr_accuracy = lr_model.score(X_test_scaled, y_test) * 100\n",
    "print(f\"📊 逻辑回归测试准确率: {lr_accuracy:.2f}%\")\n",
    "\n",
    "# 获取神经网络的最终准确率\n",
    "nn_accuracy = val_accuracies[-1]\n",
    "print(f\"🧠 神经网络测试准确率: {nn_accuracy:.2f}%\")\n",
    "\n",
    "# 对比结果\n",
    "difference = nn_accuracy - lr_accuracy\n",
    "print(f\"\\n🏆 性能对比:\")\n",
    "if difference > 0:\n",
    "    print(f\"神经网络比逻辑回归准确率高了 {difference:.2f}%! 🎉\")\n",
    "elif difference < 0:\n",
    "    print(f\"逻辑回归比神经网络准确率高了 {abs(difference):.2f}%\")\n",
    "else:\n",
    "    print(\"两种方法准确率相同\")\n",
    "\n",
    "# 思考：为什么神经网络可能比逻辑回归表现更好？\n",
    "# 提示：神经网络能学习更复杂的模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第十四步：保存模型 - 把学到的知识存起来\n",
    "\n",
    "训练好的模型很宝贵，我们要把它保存下来，以后可以直接使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型和相关信息\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),      # 模型参数\n",
    "    'optimizer_state_dict': optimizer.state_dict(),  # 优化器状态\n",
    "    'scaler': scaler,                            # 数据标准化器\n",
    "    'input_size': input_size                     # 输入特征数量\n",
    "}, 'titanic_pytorch_model.pth')\n",
    "\n",
    "print(\"💾 模型已保存为 'titanic_pytorch_model.pth'\")\n",
    "print(\"\\n🎉 项目完成！\")\n",
    "print(\"=\" * 50)\n",
    "print(\"🌟 总结：\")\n",
    "print(\"✓ 我们学会了如何处理和清洗数据\")\n",
    "print(\"✓ 我们构建了一个神经网络模型\")\n",
    "print(\"✓ 我们训练并评估了模型\")\n",
    "print(\"✓ 我们对比了不同方法的性能\")\n",
    "print(\"✓ 我们保存了训练好的模型\")\n",
    "print(\"\\n🔮 下一步可以尝试：\")\n",
    "print(\"- 调整网络结构（增加或减少层数）\")\n",
    "print(\"- 改变学习率\")\n",
    "print(\"- 尝试不同的优化器\")\n",
    "print(\"- 添加更多特征\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
