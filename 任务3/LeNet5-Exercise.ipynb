{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNISTæ‰‹å†™æ•°å­—è¯†åˆ« - PyTorchå®ç°\n",
    "\n",
    "## é¡¹ç›®ä»‹ç»\n",
    "\n",
    "åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬å°†å®ç°ä¸€ä¸ªå·ç§¯ç¥ç»ç½‘ç»œ(CNN)åˆ†ç±»æ¨¡å‹ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ç»å…¸çš„MNISTæ‰‹å†™æ•°å­—æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«äº†ï¼š\n",
    "- 0-9çš„æ‰‹å†™æ•°å­—ç°åº¦å›¾åƒï¼ˆ28Ã—28åƒç´ ï¼‰\n",
    "- 60,000ä¸ªè®­ç»ƒæ ·æœ¬å’Œ10,000ä¸ªæµ‹è¯•æ ·æœ¬\n",
    "- å¯¹åº”çš„æ•°å­—æ ‡ç­¾ï¼ˆ0-9ï¼‰â† è¿™æ˜¯æˆ‘ä»¬è¦é¢„æµ‹çš„ç›®æ ‡ï¼\n",
    "\n",
    "### èƒŒæ™¯çŸ¥è¯†\n",
    "- **MNISTæ•°æ®é›†**ï¼šè®¡ç®—æœºè§†è§‰å’Œæœºå™¨å­¦ä¹ é¢†åŸŸæœ€è‘—åçš„åŸºå‡†æ•°æ®é›†ä¹‹ä¸€\n",
    "- **å·ç§¯ç¥ç»ç½‘ç»œ(CNN)**ï¼šä¸“é—¨ç”¨äºå¤„ç†å›¾åƒæ•°æ®çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æå–å›¾åƒç‰¹å¾\n",
    "- **PyTorch**ï¼šç”±Facebookå¼€å‘çš„æµè¡Œæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæä¾›çµæ´»çš„å¼ é‡è®¡ç®—å’Œç¥ç»ç½‘ç»œæ„å»º\n",
    "\n",
    "### æˆ‘ä»¬è¦åšä»€ä¹ˆï¼Ÿ\n",
    "æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªCNNæ¨¡å‹ï¼Œå­¦ä¹ è¯†åˆ«æ‰‹å†™æ•°å­—å›¾åƒï¼Œå®ç°é«˜ç²¾åº¦çš„æ•°å­—åˆ†ç±»ã€‚\n",
    "\n",
    "\n",
    "### å…³é”®æ­¥éª¤æ¦‚è¿°ï¼š\n",
    "1. **æ•°æ®å‡†å¤‡**ï¼šä¸‹è½½MNISTæ•°æ®é›†ï¼Œè½¬æ¢ä¸ºPyTorchå¼ é‡\n",
    "2. **ç½‘ç»œè®¾è®¡**ï¼šåŒ…å«å·ç§¯å±‚ã€æ± åŒ–å±‚ã€å…¨è¿æ¥å±‚çš„CNNæ¶æ„\n",
    "3. **è®­ç»ƒå¾ªç¯**ï¼šå‰å‘ä¼ æ’­ã€æŸå¤±è®¡ç®—ã€åå‘ä¼ æ’­ã€å‚æ•°æ›´æ–°\n",
    "4. **æ€§èƒ½éªŒè¯**ï¼šè®¡ç®—æµ‹è¯•é›†å‡†ç¡®ç‡ï¼Œå¯è§†åŒ–é¢„æµ‹ç»“æœ\n",
    "\n",
    "è¿™ä¸ªé¡¹ç›®å°†å¸®åŠ©ä½ æŒæ¡ä½¿ç”¨PyTorchæ„å»ºCNNæ¨¡å‹è§£å†³å®é™…å›¾åƒåˆ†ç±»é—®é¢˜çš„å®Œæ•´æµç¨‹ã€‚\n",
    "\n",
    "## <font color='red'><b>ğŸ” è°ƒè¯•ä»»åŠ¡è¯´æ˜ï¼ˆå¯æŒ‰å¦‚ä¸‹æç¤ºä¿®æ”¹ç¬¬4æ­¥å’Œç¬¬5æ­¥ï¼Œå…¶å®ƒæ­¥éª¤ä¿æŒä¸å˜ï¼‰</b></font>\n",
    "\n",
    "<font color='red'><b>ä»»åŠ¡ç›®æ ‡ï¼šè¿™ä¸ªé¡¹ç›®éœ€è¦å®Œæˆä»¥ä¸‹æ ¸å¿ƒæ­¥éª¤ï¼š</b></font>\n",
    "<br>\n",
    "<font color='red'><b>1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†ï¼šåŠ è½½MNISTæ•°æ®é›†å¹¶è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†</b></font>\n",
    "<br>\n",
    "<font color='red'><b>2. æ¨¡å‹æ„å»ºï¼šä¿®æ”¹ç¬¬4æ­¥çš„ç¥ç»ç½‘ç»œæ¨¡å‹ç»“æ„ï¼Œè®¾è®¡åˆé€‚çš„CNNç½‘ç»œæ¶æ„</b></font>\n",
    "<br>\n",
    "<font color='red'><b>3. è®­ç»ƒä¼˜åŒ–ï¼šä¿®æ”¹ç¬¬5æ­¥æ¨¡å‹è®­ç»ƒè¶…å‚æ•°ï¼Œè®­ç»ƒæ¨¡å‹</b></font>\n",
    "<br>\n",
    "<font color='red'><b>4. è¯„ä¼°æµ‹è¯•ï¼šåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œç›®æ ‡å‡†ç¡®ç‡è¾¾åˆ°99%ä»¥ä¸Š</b></font>\n",
    "\n",
    "è¯„åˆ†è§„åˆ™è¯´æ˜ï¼šæµ‹è¯•å‡†ç¡®åº¦åœ¨\n",
    "\n",
    "96%-97%ä¹‹é—´ï¼Œç»™5åˆ†ï¼›\n",
    "<br>\n",
    "97%-98%ä¹‹é—´ï¼Œç»™10åˆ†ï¼›\n",
    "<br>\n",
    "98%-98.5%ä¹‹é—´ï¼Œç»™15åˆ†ï¼›\n",
    "<br>\n",
    "98.5%-99%ä¹‹é—´ï¼Œç»™20åˆ†ï¼›\n",
    "<br>\n",
    "è¶…è¿‡99%ï¼Œç»™25åˆ†ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒå‡†å¤‡å’Œå¯¼å…¥åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é‡æ–°å®‰è£…ï¼ˆé€‰æ‹©é€‚åˆä½ ç¯å¢ƒçš„ç‰ˆæœ¬ï¼‰\n",
    "# å¯¹äº CPU ç‰ˆæœ¬\n",
    "\n",
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import platform\n",
    "\n",
    "# æœºå™¨å­¦ä¹ å·¥å…·\n",
    "from sklearn.model_selection import train_test_split  # åˆ†å‰²æ•°æ®é›†\n",
    "from sklearn.preprocessing import StandardScaler  # æ•°æ®æ ‡å‡†åŒ–\n",
    "from sklearn.metrics import classification_report  # è¯„ä¼°æŠ¥å‘Š\n",
    "\n",
    "# ç”»å›¾å·¥å…· - ç”¨æ¥å¯è§†åŒ–ç»“æœ\n",
    "import matplotlib.pyplot as plt  \n",
    "import matplotlib\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º - è§£å†³ä¸­æ–‡ä¹±ç é—®é¢˜\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾\n",
    "plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·\n",
    "\n",
    "# è®¾ç½®å›¾åƒå¤§å°å’Œåˆ†è¾¨ç‡\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# åœ¨éœ€è¦ä½¿ç”¨æ—¶å†å¯¼å…¥ torchvision\n",
    "def get_torchvision_modules():\n",
    "    from torchvision import datasets, transforms\n",
    "    return datasets, transforms\n",
    "\n",
    "# ä½¿ç”¨æ—¶è°ƒç”¨\n",
    "datasets, transforms = get_torchvision_modules()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æ£€æŸ¥ç¯å¢ƒå’Œè®¾å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰“å°ç‰ˆæœ¬ä¿¡æ¯ï¼Œç¡®ä¿ç¯å¢ƒæ­£ç¡®\n",
    "print(f'PyTorchç‰ˆæœ¬: {torch.__version__}')\n",
    "print(f'Pythonç‰ˆæœ¬: {platform.python_version()}')\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦æœ‰GPUå¯ç”¨ï¼Œå¦‚æœæœ‰å°±ç”¨GPUï¼Œå¦åˆ™ç”¨CPU\n",
    "# GPUæ¯”CPUè®¡ç®—é€Ÿåº¦å¿«å¾ˆå¤šï¼Œé€‚åˆæ·±åº¦å­¦ä¹ è®­ç»ƒ\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'ä½¿ç”¨è®¾å¤‡: {device}')\n",
    "\n",
    "if device == 'cuda':\n",
    "    print(f'CUDAç‰ˆæœ¬: {torch.version.cuda}')\n",
    "    print(f'GPUåç§°: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MNISTæ•°æ®å‡†å¤‡\n",
    "\n",
    "MNISTæ•°æ®é›†åŒ…å«6ä¸‡å¼ æ‰‹å†™æ•°å­—å›¾ç‰‡ï¼Œæ¯å¼ å›¾ç‰‡éƒ½æ˜¯28x28åƒç´ çš„ç°åº¦å›¾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 æ•°æ®é¢„å¤„ç†å’ŒåŠ è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰æ•°æ®é¢„å¤„ç†è½¬æ¢\n",
    "# transforms.ToTensor()ï¼šå°†å›¾ç‰‡è½¬æ¢ä¸ºPyTorchå¼ é‡ï¼Œå¹¶è‡ªåŠ¨å½’ä¸€åŒ–åˆ°[0,1]èŒƒå›´\n",
    "# transforms.Normalize()ï¼šå¯¹å¼ é‡è¿›è¡Œæ ‡å‡†åŒ–ï¼Œä½¿æ•°æ®åˆ†å¸ƒæ›´å‡åŒ€ï¼Œæœ‰åŠ©äºæ¨¡å‹è®­ç»ƒ\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # å°†[0,1]èŒƒå›´è½¬æ¢ä¸º[-1,1]èŒƒå›´\n",
    "])\n",
    "\n",
    "# ä¸‹è½½å¹¶åŠ è½½è®­ç»ƒæ•°æ®é›†\n",
    "# train=Trueè¡¨ç¤ºåŠ è½½è®­ç»ƒé›†ï¼Œdownload=Trueè¡¨ç¤ºå¦‚æœæœ¬åœ°æ²¡æœ‰å°±è‡ªåŠ¨ä¸‹è½½\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# ä¸‹è½½å¹¶åŠ è½½æµ‹è¯•æ•°æ®é›†\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "print(f'è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_dataset)}')\n",
    "print(f'æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 åˆ›å»ºæ•°æ®åŠ è½½å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºæ•°æ®åŠ è½½å™¨(DataLoader)\n",
    "# DataLoaderå¯ä»¥è‡ªåŠ¨åˆ†æ‰¹(batch)åŠ è½½æ•°æ®ï¼Œå¹¶æ”¯æŒæ•°æ®æ‰“ä¹±(shuffle)\n",
    "# batch_size=100ï¼šæ¯æ‰¹å¤„ç†100å¼ å›¾ç‰‡\n",
    "# shuffle=Trueï¼šæ¯ä¸ªepochå¼€å§‹æ—¶æ‰“ä¹±æ•°æ®é¡ºåº\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "print(f'è®­ç»ƒé›†æ‰¹æ¬¡æ•°: {len(train_loader)}')\n",
    "print(f'æµ‹è¯•é›†æ‰¹æ¬¡æ•°: {len(test_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 å¯è§†åŒ–æ•°æ®æ ·æœ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»æ•°æ®åŠ è½½å™¨ä¸­è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "print(f'å›¾ç‰‡å¼ é‡å½¢çŠ¶: {images.shape}')  # [batch_size, channels, height, width]\n",
    "print(f'æ ‡ç­¾å½¢çŠ¶: {labels.shape}')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def simple_display(images, labels, max_display=6):\n",
    "    \"\"\"ç®€åŒ–ç‰ˆå›¾ç‰‡æ˜¾ç¤º\"\"\"\n",
    "    try:\n",
    "        # ç¡®ä¿æ•°æ®åœ¨CPUä¸Š\n",
    "        if images.is_cuda:\n",
    "            images = images.cpu()\n",
    "            labels = labels.cpu()\n",
    "        \n",
    "        # åˆ†ç¦»æ¢¯åº¦\n",
    "        images = images.detach()\n",
    "        labels = labels.detach()\n",
    "        \n",
    "        max_display = min(max_display, images.shape[0])\n",
    "        print(f\"æ˜¾ç¤º {max_display} å¼ å›¾ç‰‡\")\n",
    "        \n",
    "        # é€ä¸ªæ˜¾ç¤ºï¼Œé¿å…å†…å­˜é—®é¢˜\n",
    "        for i in range(max_display):\n",
    "            plt.figure(figsize=(4, 4))\n",
    "            img = images[i].squeeze().numpy()\n",
    "            plt.imshow(img, cmap='gray')\n",
    "            plt.title(f'Label: {labels[i].item()}')\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"é”™è¯¯: {e}\")\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "simple_display(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ„å»ºå·ç§¯ç¥ç»ç½‘ç»œ\n",
    "<div style=\"color: #FF0000; font-size: 18px; font-weight: bold; background-color: #FFF0F0; padding: 10px; border: 2px solid #FF0000; border-radius: 5px;\"> è€ƒè¯•ä»»åŠ¡ï¼šä¿®æ”¹ä¸‹é¢LeNet(nn.Module)é‡Œçš„ç½‘ç»œæ¨¡å‹é…ç½®(ä¾‹å¦‚ä¿®æ”¹ç¥ç»ç½‘ç»œçš„å±‚æ•°æˆ–è€…å¢åŠ ç¥ç»å…ƒä¸ªæ•°ç­‰)ï¼Œä½¿å…¶æµ‹è¯•é›†å‡†ç¡®ç‡è¶…è¿‡99% </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"åˆå§‹åŒ–ç½‘ç»œå±‚\"\"\"\n",
    "        super(LeNet, self).__init__()  # è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°\n",
    "        \n",
    "        # ç¬¬ä¸€ä¸ªå·ç§¯å±‚\n",
    "        # è¾“å…¥é€šé“æ•°=1(ç°åº¦å›¾)ï¼Œè¾“å‡ºé€šé“æ•°=6ï¼Œå·ç§¯æ ¸å¤§å°=5x5ï¼Œå¡«å……=2\n",
    "        # å¡«å……(padding)æ˜¯ä¸ºäº†ä¿æŒç‰¹å¾å›¾å°ºå¯¸ä¸å˜\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, padding=2)\n",
    "        \n",
    "        # ç¬¬äºŒä¸ªå·ç§¯å±‚\n",
    "        # è¾“å…¥é€šé“æ•°=6ï¼Œè¾“å‡ºé€šé“æ•°=16ï¼Œå·ç§¯æ ¸å¤§å°=5x5\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        # å…¨è¿æ¥å±‚\n",
    "        # 16ä¸ª5x5çš„ç‰¹å¾å›¾å±•å¹³åæ˜¯16*5*5=400ä¸ªç‰¹å¾\n",
    "        self.fc1 = nn.Linear(16*5*5, 60)  \n",
    "        self.fc2 = nn.Linear(60, 10)      \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"å‰å‘ä¼ æ’­è¿‡ç¨‹\"\"\"\n",
    "        \n",
    "        # ç¬¬ä¸€å±‚ï¼šå·ç§¯ -> ReLUæ¿€æ´» -> æœ€å¤§æ± åŒ–\n",
    "        # è¾“å…¥: (batch_size, 1, 28, 28)\n",
    "        # å·ç§¯å: (batch_size, 6, 28, 28) -> æ± åŒ–å: (batch_size, 6, 14, 14)\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        \n",
    "        # ç¬¬äºŒå±‚ï¼šå·ç§¯ -> ReLUæ¿€æ´» -> æœ€å¤§æ± åŒ–\n",
    "        # è¾“å…¥: (batch_size, 6, 14, 14)\n",
    "        # å·ç§¯å: (batch_size, 16, 10, 10) -> æ± åŒ–å: (batch_size, 16, 5, 5)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        \n",
    "        # å±•å¹³ç‰¹å¾å›¾ï¼Œä¸ºå…¨è¿æ¥å±‚åšå‡†å¤‡\n",
    "        # (batch_size, 16, 5, 5) -> (batch_size, 16*5*5=400)\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        \n",
    "        # å…¨è¿æ¥å±‚ + ReLUæ¿€æ´»\n",
    "        x = F.relu(self.fc1(x))  \n",
    "        \n",
    "        # è¾“å‡ºå±‚ï¼ˆä¸éœ€è¦æ¿€æ´»å‡½æ•°ï¼Œå› ä¸ºåé¢ä¼šç”¨CrossEntropyLossï¼‰\n",
    "        x = self.fc2(x)          \n",
    "        \n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        \"\"\"è®¡ç®—å±•å¹³åçš„ç‰¹å¾æ•°é‡\"\"\"\n",
    "        size = x.size()[1:]  # è·å–é™¤äº†batchç»´åº¦ä¹‹å¤–çš„æ‰€æœ‰ç»´åº¦\n",
    "        return np.prod(size)  # è®¡ç®—æ‰€æœ‰ç»´åº¦çš„ä¹˜ç§¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 æŸ¥çœ‹ç½‘ç»œç»“æ„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºç½‘ç»œå®ä¾‹\n",
    "net = LeNet()\n",
    "print(\"ç½‘ç»œç»“æ„:\")\n",
    "print(net)\n",
    "\n",
    "# å°†ç½‘ç»œç§»åŠ¨åˆ°è®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰\n",
    "net = net.to(device)\n",
    "print(f\"\\nç½‘ç»œå·²ç§»åŠ¨åˆ°: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 æŸ¥çœ‹ç½‘ç»œå‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹ç½‘ç»œçš„æ‰€æœ‰å¯è®­ç»ƒå‚æ•°\n",
    "print(\"ç½‘ç»œå‚æ•°:\")\n",
    "total_params = 0\n",
    "for name, param in net.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.size()} (å¯è®­ç»ƒå‚æ•°æ•°: {param.numel()})\")\n",
    "        total_params += param.numel()\n",
    "\n",
    "print(f\"\\næ€»å¯è®­ç»ƒå‚æ•°æ•°: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æ¨¡å‹è®­ç»ƒ\n",
    "\n",
    "### 5.1 å®šä¹‰è®­ç»ƒå‡½æ•°\n",
    "<div style=\"color: #FF0000; font-size: 18px; font-weight: bold; background-color: #FFF0F0; padding: 10px; border: 2px solid #FF0000; border-radius: 5px;\"> è€ƒè¯•ä»»åŠ¡ï¼šç»§ç»­ä¿®æ”¹ä¸‹é¢train_modelé‡Œçš„æ¨¡å‹è®­ç»ƒå‚æ•°ï¼Œä½¿å…¶æµ‹è¯•é›†å‡†ç¡®ç‡è¶…è¿‡99% </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, epochs=10, learning_rate=0.0005, momentum=0.9):\n",
    "    \"\"\"\n",
    "    è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹\n",
    "    \n",
    "    å‚æ•°:\n",
    "    - model: è¦è®­ç»ƒçš„ç¥ç»ç½‘ç»œæ¨¡å‹\n",
    "    - train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨\n",
    "    - test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨\n",
    "    - epochs: è®­ç»ƒè½®æ•°\n",
    "    - learning_rate: å­¦ä¹ ç‡\n",
    "    - momentum: åŠ¨é‡å‚æ•°\n",
    "    \"\"\"\n",
    "    \n",
    "    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨\n",
    "    # CrossEntropyLoss: äº¤å‰ç†µæŸå¤±ï¼Œé€‚ç”¨äºå¤šåˆ†ç±»é—®é¢˜\n",
    "    # SGD: éšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨ï¼Œå¸¦æœ‰åŠ¨é‡å¯ä»¥åŠ é€Ÿæ”¶æ•›\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    \n",
    "    # è®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å’Œå‡†ç¡®ç‡\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    print(\"å¼€å§‹è®­ç»ƒ...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # è®­ç»ƒæ¨¡å¼\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # æ¢¯åº¦æ¸…é›¶\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # å‰å‘ä¼ æ’­\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # è®¡ç®—æŸå¤±\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # åå‘ä¼ æ’­\n",
    "            loss.backward()\n",
    "            \n",
    "            # å‚æ•°æ›´æ–°\n",
    "            optimizer.step()\n",
    "            \n",
    "            # ç»Ÿè®¡ä¿¡æ¯\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # æ¯100ä¸ªbatchæ‰“å°ä¸€æ¬¡ä¿¡æ¯\n",
    "            if batch_idx % 100 == 99:\n",
    "                print(f'[Epoch: {epoch+1}, Batch: {batch_idx+1}] æŸå¤±: {running_loss/100:.3f}')\n",
    "                running_loss = 0.0\n",
    "        \n",
    "        # è®¡ç®—è®­ç»ƒå‡†ç¡®ç‡\n",
    "        train_accuracy = 100. * correct / total\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹\n",
    "        test_accuracy = evaluate_model(model, test_loader)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{epochs}] - è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.2f}%, æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.2f}%')\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f'è®­ç»ƒå®Œæˆ! æ€»è€—æ—¶: {end_time-start_time:.2f}ç§’')\n",
    "    \n",
    "    return train_accuracies, test_accuracies\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡\"\"\"\n",
    "    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # åœ¨è¯„ä¼°æ—¶ä¸éœ€è¦è®¡ç®—æ¢¯åº¦ï¼Œå¯ä»¥èŠ‚çœå†…å­˜å’Œè®¡ç®—èµ„æº\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 å¼€å§‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒæ¨¡å‹\n",
    "train_acc, test_acc = train_model(net, train_loader, test_loader, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»˜åˆ¶è®­ç»ƒå’Œæµ‹è¯•å‡†ç¡®ç‡æ›²çº¿\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_acc)+1), train_acc, 'b-', label='è®­ç»ƒå‡†ç¡®ç‡')\n",
    "plt.plot(range(1, len(test_acc)+1), test_acc, 'r-', label='æµ‹è¯•å‡†ç¡®ç‡')\n",
    "plt.xlabel('è®­ç»ƒè½®æ•°')\n",
    "plt.ylabel('å‡†ç¡®ç‡ (%)')\n",
    "plt.title('è®­ç»ƒå’Œæµ‹è¯•å‡†ç¡®ç‡')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# æ˜¾ç¤ºæœ€ç»ˆå‡†ç¡®ç‡\n",
    "final_train_acc = train_acc[-1]\n",
    "final_test_acc = test_acc[-1]\n",
    "categories = ['è®­ç»ƒå‡†ç¡®ç‡', 'æµ‹è¯•å‡†ç¡®ç‡']\n",
    "values = [final_train_acc, final_test_acc]\n",
    "colors = ['blue', 'red']\n",
    "\n",
    "bars = plt.bar(categories, values, color=colors)\n",
    "plt.ylabel('å‡†ç¡®ç‡ (%)')\n",
    "plt.title('æœ€ç»ˆå‡†ç¡®ç‡å¯¹æ¯”')\n",
    "\n",
    "# åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼\n",
    "for bar, value in zip(bars, values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f'{value:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {final_train_acc:.2f}%\")\n",
    "print(f\"æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {final_test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æ¨¡å‹æµ‹è¯•å’Œé¢„æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_image(model, test_loader, index=0):\n",
    "    \"\"\"æµ‹è¯•å•å¼ å›¾ç‰‡çš„é¢„æµ‹ç»“æœ\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # è·å–æµ‹è¯•æ•°æ®\n",
    "    test_data, test_labels = next(iter(test_loader))\n",
    "    \n",
    "    # é€‰æ‹©æŒ‡å®šç´¢å¼•çš„å›¾ç‰‡\n",
    "    image = test_data[index].unsqueeze(0).to(device)  # å¢åŠ batchç»´åº¦\n",
    "    true_label = test_labels[index].item()\n",
    "    \n",
    "    # é¢„æµ‹\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        probabilities = F.softmax(output, dim=1)  # è½¬æ¢ä¸ºæ¦‚ç‡\n",
    "        predicted_prob, predicted_label = torch.max(probabilities, 1)\n",
    "    \n",
    "    # æ˜¾ç¤ºç»“æœ\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    # æ˜¾ç¤ºå›¾ç‰‡\n",
    "    plt.subplot(1, 2, 1)\n",
    "    img = test_data[index].squeeze().numpy()\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title(f'çœŸå®æ ‡ç­¾: {true_label}')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # æ˜¾ç¤ºé¢„æµ‹æ¦‚ç‡\n",
    "    plt.subplot(1, 2, 2)\n",
    "    probs = probabilities.cpu().numpy().flatten()\n",
    "    plt.bar(range(10), probs, color='skyblue')\n",
    "    plt.xlabel('æ•°å­—ç±»åˆ«')\n",
    "    plt.ylabel('é¢„æµ‹æ¦‚ç‡')\n",
    "    plt.title(f'é¢„æµ‹ç»“æœ: {predicted_label.item()} (æ¦‚ç‡: {predicted_prob.item():.2f})')\n",
    "    plt.xticks(range(10))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"çœŸå®æ ‡ç­¾: {true_label}\")\n",
    "    print(f\"é¢„æµ‹æ ‡ç­¾: {predicted_label.item()}\")\n",
    "    print(f\"é¢„æµ‹æ¦‚ç‡: {predicted_prob.item():.4f}\")\n",
    "    \n",
    "    if true_label == predicted_label.item():\n",
    "        print(\"âœ… é¢„æµ‹æ­£ç¡®!\")\n",
    "    else:\n",
    "        print(\"âŒ é¢„æµ‹é”™è¯¯!\")\n",
    "\n",
    "# æµ‹è¯•å‡ å¼ å›¾ç‰‡\n",
    "for i in range(3):\n",
    "    test_single_image(net, test_loader, index=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ä¿å­˜å’ŒåŠ è½½æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹\n",
    "torch.save(net.state_dict(), 'lenet_mnist.pth')\n",
    "print(\"æ¨¡å‹å·²ä¿å­˜ä¸º 'lenet_mnist.pth'\")\n",
    "\n",
    "# åŠ è½½æ¨¡å‹çš„ç¤ºä¾‹ï¼ˆåœ¨å®é™…ä½¿ç”¨æ—¶ï¼‰\n",
    "# 1. åˆ›å»ºæ–°çš„ç½‘ç»œå®ä¾‹\n",
    "# new_net = LeNet().to(device)\n",
    "# 2. åŠ è½½ä¿å­˜çš„å‚æ•°\n",
    "# new_net.load_state_dict(torch.load('lenet_mnist.pth'))\n",
    "# 3. è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "# new_net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. æ€»ç»“\n",
    "\n",
    "### æœ¬æ•™ç¨‹å­¦åˆ°çš„çŸ¥è¯†ç‚¹:\n",
    "1. **ç¯å¢ƒé…ç½®**: æ£€æŸ¥PyTorchç‰ˆæœ¬å’ŒGPUå¯ç”¨æ€§\n",
    "2. **æ•°æ®å‡†å¤‡**: ä¸‹è½½ã€é¢„å¤„ç†å’ŒåŠ è½½MNISTæ•°æ®é›†\n",
    "3. **æ•°æ®å¯è§†åŒ–**: ä½¿ç”¨matplotlibæ˜¾ç¤ºå›¾ç‰‡\n",
    "4. **ç½‘ç»œæ„å»º**: å®šä¹‰LeNet-5å·ç§¯ç¥ç»ç½‘ç»œ\n",
    "5. **æ¨¡å‹è®­ç»ƒ**: ä½¿ç”¨SGDä¼˜åŒ–å™¨å’Œäº¤å‰ç†µæŸå¤±å‡½æ•°\n",
    "6. **æ¨¡å‹è¯„ä¼°**: è®¡ç®—è®­ç»ƒå’Œæµ‹è¯•å‡†ç¡®ç‡\n",
    "7. **ç»“æœå¯è§†åŒ–**: ç»˜åˆ¶å‡†ç¡®ç‡æ›²çº¿å’Œé¢„æµ‹ç»“æœ\n",
    "8. **æ¨¡å‹ä¿å­˜**: ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°\n",
    "\n",
    "### è¿›ä¸€æ­¥å­¦ä¹ çš„å»ºè®®:\n",
    "1. å°è¯•è°ƒæ•´ç½‘ç»œç»“æ„ï¼ˆå¢åŠ å±‚æ•°ã€æ”¹å˜é€šé“æ•°ï¼‰\n",
    "2. å®éªŒä¸åŒçš„ä¼˜åŒ–å™¨ï¼ˆAdamã€RMSpropç­‰ï¼‰\n",
    "3. è°ƒæ•´è¶…å‚æ•°ï¼ˆå­¦ä¹ ç‡ã€æ‰¹å¤§å°ã€è®­ç»ƒè½®æ•°ï¼‰\n",
    "4. å°è¯•å…¶ä»–æ•°æ®é›†ï¼ˆå¦‚Fashion-MNISTã€CIFAR-10ï¼‰\n",
    "5. æ·»åŠ æ­£åˆ™åŒ–æŠ€æœ¯ï¼ˆDropoutã€æƒé‡è¡°å‡ï¼‰é˜²æ­¢è¿‡æ‹Ÿåˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡\n",
    "final_accuracy = evaluate_model(net, test_loader)\n",
    "print(f\"ğŸ‰ æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æœ€ç»ˆå‡†ç¡®ç‡: {final_accuracy:.2f}%\")\n",
    "print(\"\\næ­å–œä½ å®Œæˆäº†ç¬¬ä¸€ä¸ªå·ç§¯ç¥ç»ç½‘ç»œçš„è®­ç»ƒ! ğŸš€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
