{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归模型调试任务\n",
    "\n",
    "## 欢迎来到AI世界！\n",
    "\n",
    "在这个考试任务中，我们将一起调试一个线性回归模型。我们将使用加利福尼亚房价数据集，该数据集包含了如收入中位数、房龄、房间数量、地理位置等8个特征，我们的目标是构建一个模型来准确预测区域的房价中位数。线性回归是机器学习中最基础的算法之一，就像学习数学要先学会加减法一样重要！\n",
    "\n",
    "### 我们的目标：\n",
    "- 理解线性回归的基本原理\n",
    "- 学会使用梯度下降算法\n",
    "- 掌握PyTorch的基本操作\n",
    "- 通过调试找出代码中的问题并修复\n",
    "\n",
    "## <font color='red'><b>🔍 调试任务说明</b></font>\n",
    "\n",
    "<font color='red'><b>任务目标：这个项目中有几处代码需要调试和修复。你的任务是：</b></font>\n",
    "<br>\n",
    "<font color='red'><b>1. 逐步执行此文档里的每一步代码</b></font>\n",
    "<br>\n",
    "<font color='red'><b>2. 理解代码的含义</b></font>\n",
    "<br>\n",
    "<font color='red'><b>3. 根据第6步和第7步代码的提示，修改相应的参数，使其达到任务设定值</b></font>\n",
    "\n",
    "评分规则说明：验证准确率在\n",
    "<br>\n",
    "只完成任务1，未完成任务2，给10分；\n",
    "<br>\n",
    "只完成任务2，未完成任务1，给15分；\n",
    "<br>\n",
    "完成任务1和任务2，给25分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一步：导入必要的工具包"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T01:09:52.296130Z",
     "start_time": "2025-11-17T01:09:52.292129Z"
    }
   },
   "source": [
    "import numpy as np  # 数学计算工具箱\n",
    "import pandas as pd  # 数据处理工具箱\n",
    "import torch  # 深度学习框架\n",
    "import torch.nn as nn  # 神经网络模块\n",
    "import matplotlib.pyplot as plt  # 绘图工具箱\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 设置图像大小和分辨率\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"所有工具包导入成功！\")\n",
    "print(f\"PyTorch版本: {torch.__version__}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有工具包导入成功！\n",
      "PyTorch版本: 2.8.0+cpu\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二步：加载和了解数据\n",
    "\n",
    "我们将使用波士顿房价数据集，这是一个经典的回归问题数据集。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T01:09:53.010970Z",
     "start_time": "2025-11-17T01:09:52.996970Z"
    }
   },
   "source": [
    "# 使用加利福尼亚房价数据集作为替代\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 加载加利福尼亚房价数据集\n",
    "california = fetch_california_housing()\n",
    "X, y = california.data, california.target\n",
    "\n",
    "# 查看数据集信息\n",
    "print(\"特征名称:\", california.feature_names)\n",
    "print(\"数据形状:\", X.shape)\n",
    "print(\"目标变量形状:\", y.shape)\n",
    "\n",
    "# 转换为DataFrame便于查看\n",
    "df = pd.DataFrame(X, columns=california.feature_names)\n",
    "df['Price'] = y\n",
    "print(df.head())\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 查看数据的基本信息\n",
    "print(\"=== 数据基本信息 ===\")\n",
    "print(f\"训练集特征形状: {X_train.shape}\")\n",
    "print(f\"训练集标签形状: {y_train.shape}\")\n",
    "print(f\"测试集特征形状: {X_test.shape}\")\n",
    "print(f\"测试集标签形状: {y_test.shape}\")\n",
    "print(f\"特征数量: {X_train.shape[1]}\")\n",
    "print(f\"特征名称: {california.feature_names}\")  # 移除了 .tolist()\n",
    "\n",
    "# 显示前5个样本\n",
    "print(\"\\n=== 前5个样本的特征值 ===\")\n",
    "for i in range(5):\n",
    "    print(f\"样本 {i+1}: {X_train[i]} -> 价格: {y_train[i]:.2f}千美元\")\n",
    "\n",
    "# 数据统计信息\n",
    "print(\"\\n=== 数据统计信息 ===\")\n",
    "print(f\"房价范围: {y.min():.2f} ~ {y.max():.2f}千美元\")\n",
    "print(f\"平均房价: {y.mean():.2f}千美元\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征名称: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      "数据形状: (20640, 8)\n",
      "目标变量形状: (20640,)\n",
      "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
      "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
      "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
      "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "\n",
      "   Longitude  Price  \n",
      "0    -122.23  4.526  \n",
      "1    -122.22  3.585  \n",
      "2    -122.24  3.521  \n",
      "3    -122.25  3.413  \n",
      "4    -122.25  3.422  \n",
      "=== 数据基本信息 ===\n",
      "训练集特征形状: (16512, 8)\n",
      "训练集标签形状: (16512,)\n",
      "测试集特征形状: (4128, 8)\n",
      "测试集标签形状: (4128,)\n",
      "特征数量: 8\n",
      "特征名称: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      "\n",
      "=== 前5个样本的特征值 ===\n",
      "样本 1: [ 3.25960000e+00  3.30000000e+01  5.01765650e+00  1.00642055e+00\n",
      "  2.30000000e+03  3.69181380e+00  3.27100000e+01 -1.17030000e+02] -> 价格: 1.03千美元\n",
      "样本 2: [ 3.81250000e+00  4.90000000e+01  4.47354497e+00  1.04100529e+00\n",
      "  1.31400000e+03  1.73809524e+00  3.37700000e+01 -1.18160000e+02] -> 价格: 3.82千美元\n",
      "样本 3: [   4.1563        4.            5.64583333    0.98511905  915.\n",
      "    2.72321429   34.66       -120.48      ] -> 价格: 1.73千美元\n",
      "样本 4: [ 1.94250000e+00  3.60000000e+01  4.00281690e+00  1.03380282e+00\n",
      "  1.41800000e+03  3.99436620e+00  3.26900000e+01 -1.17110000e+02] -> 价格: 0.93千美元\n",
      "样本 5: [   3.5542       43.            6.26842105    1.13421053  874.\n",
      "    2.3          36.78       -119.8       ] -> 价格: 0.96千美元\n",
      "\n",
      "=== 数据统计信息 ===\n",
      "房价范围: 0.15 ~ 5.00千美元\n",
      "平均房价: 2.07千美元\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三步：数据预处理\n",
    "\n",
    "为了让模型更好地学习，我们需要对数据进行标准化处理。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T01:09:53.311067Z",
     "start_time": "2025-11-17T01:09:53.300067Z"
    }
   },
   "source": [
    "# 数据标准化 - 这是一个重要的步骤！\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 创建标准化器\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "# 对训练集进行标准化\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "\n",
    "# 对测试集进行标准化（使用训练集的参数）\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.FloatTensor(y_train_scaled)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.FloatTensor(y_test_scaled)\n",
    "\n",
    "print(\"=== 数据预处理完成 ===\")\n",
    "print(f\"标准化后的训练集特征形状: {X_train_tensor.shape}\")\n",
    "print(f\"标准化后的训练集标签形状: {y_train_tensor.shape}\")\n",
    "print(f\"标准化后的特征均值: {X_train_tensor.mean():.4f}\")\n",
    "print(f\"标准化后的特征标准差: {X_train_tensor.std():.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 数据预处理完成 ===\n",
      "标准化后的训练集特征形状: torch.Size([16512, 8])\n",
      "标准化后的训练集标签形状: torch.Size([16512])\n",
      "标准化后的特征均值: 0.0000\n",
      "标准化后的特征标准差: 1.0000\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四步：理解线性回归原理\n",
    "\n",
    "### 线性回归公式：\n",
    "$$ y = w_1x_1 + w_2x_2 + ... + w_nx_n + b $$\n",
    "\n",
    "其中：\n",
    "- $x_1, x_2, ..., x_n$ 是特征（输入）\n",
    "- $w_1, w_2, ..., w_n$ 是权重（需要学习的参数）\n",
    "- $b$ 是偏置项\n",
    "- $y$ 是预测值（输出）"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T01:09:53.625323Z",
     "start_time": "2025-11-17T01:09:53.606322Z"
    }
   },
   "source": [
    "# 函数1：手动实现线性回归预测函数\n",
    "def linear_regression_predict(X, weights, bias):\n",
    "    \"\"\"\n",
    "    手动实现线性回归预测\n",
    "    \n",
    "    参数:\n",
    "    X: 输入特征，形状为 (样本数, 特征数)\n",
    "    weights: 权重，形状为 (特征数,)\n",
    "    bias: 偏置，标量\n",
    "    \n",
    "    返回:\n",
    "    y_pred: 预测值，形状为 (样本数,)\n",
    "    \"\"\"\n",
    "    # TODO: 实现线性回归的预测公式\n",
    "    # 提示：使用 torch.matmul 或 @ 运算符进行矩阵乘法\n",
    "    # 注意：这里有一个需要调试的问题\n",
    "    y_pred = torch.matmul(X, weights) + bias  # 请检查这行代码是否有问题\n",
    "    return y_pred\n",
    "\n",
    "# 测试预测函数\n",
    "sample_weights = torch.randn(X_train_tensor.shape[1])  # 随机初始化权重\n",
    "sample_bias = torch.randn(1)  # 随机初始化偏置\n",
    "\n",
    "predictions = linear_regression_predict(X_train_tensor, sample_weights, sample_bias)\n",
    "print(f\"预测值形状: {predictions.shape}\")\n",
    "print(f\"前5个预测值: {predictions[:5]}\")\n",
    "print(f\"对应的真实值: {y_train_tensor[:5]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测值形状: torch.Size([16512])\n",
      "前5个预测值: tensor([-0.8671, -3.0281,  2.9226, -2.4552, -2.3297])\n",
      "对应的真实值: tensor([-0.9012,  1.5128, -0.2992, -0.9842, -0.9574])\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第五步：理解损失函数\n",
    "\n",
    "### 均方误差（MSE）公式：\n",
    "$$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "其中：\n",
    "- $y_i$ 是真实值\n",
    "- $\\hat{y}_i$ 是预测值\n",
    "- $n$ 是样本数量"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T01:09:53.910188Z",
     "start_time": "2025-11-17T01:09:53.901133Z"
    }
   },
   "source": [
    "# 函数2：手动实现均方误差损失函数\n",
    "def mse_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    计算均方误差损失\n",
    "    \n",
    "    参数:\n",
    "    y_true: 真实值，形状为 (样本数,)\n",
    "    y_pred: 预测值，形状为 (样本数,)\n",
    "    \n",
    "    返回:\n",
    "    loss: 均方误差损失，标量\n",
    "    \"\"\"\n",
    "    # TODO: 实现均方误差公式\n",
    "    # 提示：先计算差值，然后平方，最后求平均\n",
    "    # 注意：这里有一个需要调试的问题\n",
    "    loss = torch.mean((y_true - y_pred) ** 2)  # 请检查这行代码是否有问题\n",
    "    return loss\n",
    "\n",
    "# 测试损失函数\n",
    "test_loss = mse_loss(y_train_tensor, predictions)\n",
    "print(f\"当前模型的损失: {test_loss:.4f}\")\n",
    "\n",
    "# 使用PyTorch内置函数验证\n",
    "torch_loss = nn.MSELoss()(predictions, y_train_tensor)\n",
    "print(f\"PyTorch内置损失函数结果: {torch_loss:.4f}\")\n",
    "print(f\"我们的实现是否正确: {torch.abs(test_loss - torch_loss) < 1e-6}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前模型的损失: 6.6775\n",
      "PyTorch内置损失函数结果: 6.6775\n",
      "我们的实现是否正确: True\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第六步：采用梯度下降对线性回归模型进行参数训练\n",
    "\n",
    "梯度下降是通过计算损失函数对参数的梯度（导数），然后沿着梯度反方向更新参数来最小化损失函数的过程。\n",
    "\n",
    "### 参数更新公式：\n",
    "$$ w = w - \\alpha \\cdot \\frac{\\partial L}{\\partial w} $$\n",
    "$$ b = b - \\alpha \\cdot \\frac{\\partial L}{\\partial b} $$\n",
    "\n",
    "其中：\n",
    "- $\\alpha$ 是学习率\n",
    "- $\\frac{\\partial L}{\\partial w}$ 是损失对权重的梯度\n",
    "- $\\frac{\\partial L}{\\partial b}$ 是损失对偏置的梯度\n",
    "\n",
    "<div style=\"color: #FF0000; font-size: 18px; font-weight: bold; background-color: #FFF0F0; padding: 10px; border: 2px solid #FF0000; border-radius: 5px;\"> 考试任务1：下面的函数3中，其它参数不变，修改学习率，使得最后的损失函数值在0.4以下 </div>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T01:10:09.341877Z",
     "start_time": "2025-11-17T01:10:09.322877Z"
    }
   },
   "source": [
    "# 函数3：手动实现梯度下降\n",
    "def manual_gradient_descent(X, y, weights, bias, learning_rate, epochs):\n",
    "    \"\"\"\n",
    "    手动实现梯度下降\n",
    "    \n",
    "    参数:\n",
    "    X: 输入特征\n",
    "    y: 真实标签\n",
    "    weights: 初始权重\n",
    "    bias: 初始偏置\n",
    "    learning_rate: 学习率\n",
    "    epochs: 训练轮数\n",
    "    \n",
    "    返回:\n",
    "    weights: 训练后的权重\n",
    "    bias: 训练后的偏置\n",
    "    losses: 每轮的损失记录\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # 前向传播：计算预测值\n",
    "        y_pred = linear_regression_predict(X, weights, bias)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = mse_loss(y, y_pred)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # 手动计算梯度\n",
    "        # 对于线性回归，梯度的计算公式为：\n",
    "        # dL/dw = (2/n) * X^T @ (y_pred - y)\n",
    "        # dL/db = (2/n) * sum(y_pred - y)\n",
    "        \n",
    "        n = len(y)\n",
    "        error = y_pred - y\n",
    "        \n",
    "        # TODO: 计算权重和偏置的梯度\n",
    "        # 注意：这里有两个需要调试的问题\n",
    "        grad_weights = (2/n) * torch.matmul(X.T, error)  # 请检查这行代码\n",
    "        grad_bias = (2/n) * torch.sum(error)  # 请检查这行代码\n",
    "        \n",
    "        # 更新参数\n",
    "        weights = weights - learning_rate * grad_weights\n",
    "        bias = bias - learning_rate * grad_bias\n",
    "        \n",
    "        if epoch % 2 == 0:\n",
    "            print(f\"轮次 {epoch}, 损失: {loss:.4f}\")\n",
    "    \n",
    "    return weights, bias, losses\n",
    "\n",
    "# 测试手动梯度下降\n",
    "print(\"=== 开始手动梯度下降训练 ===\")\n",
    "manual_weights = torch.randn(X_train_tensor.shape[1], requires_grad=False)\n",
    "manual_bias = torch.randn(1, requires_grad=False)\n",
    "\n",
    "final_weights, final_bias, manual_losses = manual_gradient_descent(\n",
    "    X_train_tensor, y_train_tensor, manual_weights, manual_bias,\n",
    "    # todo: 修改学习率到0.3，然后多跑几次找到最好成绩\n",
    "    learning_rate=0.3, epochs=10\n",
    ")\n",
    "\n",
    "print(f\"\\n训练完成！最终损失: {manual_losses[-1]:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 开始手动梯度下降训练 ===\n",
      "轮次 0, 损失: 2.4012\n",
      "轮次 2, 损失: 0.4410\n",
      "轮次 4, 损失: 0.4024\n",
      "轮次 6, 损失: 0.3968\n",
      "轮次 8, 损失: 0.3948\n",
      "\n",
      "训练完成！最终损失: 0.3941\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #FF0000; font-size: 18px; font-weight: bold; background-color: #FFF0F0; padding: 10px; border: 2px solid #FF0000; border-radius: 5px;\"> 考试任务2：下面的函数4中，其它参数不变，修改训练轮数，使得最后的损失函数值在0.4以下 </div>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T01:10:34.017904Z",
     "start_time": "2025-11-17T01:10:33.788130Z"
    }
   },
   "source": [
    "# 函数4：手动实现梯度下降\n",
    "def manual_gradient_descent(X, y, weights, bias, learning_rate, epochs):\n",
    "    \"\"\"\n",
    "    手动实现梯度下降\n",
    "    \n",
    "    参数:\n",
    "    X: 输入特征\n",
    "    y: 真实标签\n",
    "    weights: 初始权重\n",
    "    bias: 初始偏置\n",
    "    learning_rate: 学习率\n",
    "    epochs: 训练轮数\n",
    "    \n",
    "    返回:\n",
    "    weights: 训练后的权重\n",
    "    bias: 训练后的偏置\n",
    "    losses: 每轮的损失记录\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # 前向传播：计算预测值\n",
    "        y_pred = linear_regression_predict(X, weights, bias)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = mse_loss(y, y_pred)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # 手动计算梯度\n",
    "        # 对于线性回归，梯度的计算公式为：\n",
    "        # dL/dw = (2/n) * X^T @ (y_pred - y)\n",
    "        # dL/db = (2/n) * sum(y_pred - y)\n",
    "        \n",
    "        n = len(y)\n",
    "        error = y_pred - y\n",
    "        \n",
    "        # TODO: 计算权重和偏置的梯度\n",
    "        # 注意：这里有两个需要调试的问题\n",
    "        grad_weights = (2/n) * torch.matmul(X.T, error)  # 请检查这行代码\n",
    "        grad_bias = (2/n) * torch.sum(error)  # 请检查这行代码\n",
    "        \n",
    "        # 更新参数\n",
    "        weights = weights - learning_rate * grad_weights\n",
    "        bias = bias - learning_rate * grad_bias\n",
    "        \n",
    "        if epoch % 2 == 0:\n",
    "            print(f\"轮次 {epoch}, 损失: {loss:.4f}\")\n",
    "    \n",
    "    return weights, bias, losses\n",
    "\n",
    "# 测试手动梯度下降\n",
    "print(\"=== 开始手动梯度下降训练 ===\")\n",
    "manual_weights = torch.randn(X_train_tensor.shape[1], requires_grad=False)\n",
    "manual_bias = torch.randn(1, requires_grad=False)\n",
    "\n",
    "final_weights, final_bias, manual_losses = manual_gradient_descent(\n",
    "    X_train_tensor, y_train_tensor, manual_weights, manual_bias,\n",
    "    # todo: 修改训练轮数到1000, 大力出奇迹\n",
    "    learning_rate=0.01, epochs=1000\n",
    ")\n",
    "\n",
    "print(f\"\\n训练完成！最终损失: {manual_losses[-1]:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 开始手动梯度下降训练 ===\n",
      "轮次 0, 损失: 15.4763\n",
      "轮次 2, 损失: 13.9491\n",
      "轮次 4, 损失: 12.5886\n",
      "轮次 6, 损失: 11.3753\n",
      "轮次 8, 损失: 10.2923\n",
      "轮次 10, 损失: 9.3248\n",
      "轮次 12, 损失: 8.4597\n",
      "轮次 14, 损失: 7.6853\n",
      "轮次 16, 损失: 6.9917\n",
      "轮次 18, 损失: 6.3698\n",
      "轮次 20, 损失: 5.8117\n",
      "轮次 22, 损失: 5.3106\n",
      "轮次 24, 损失: 4.8601\n",
      "轮次 26, 损失: 4.4549\n",
      "轮次 28, 损失: 4.0901\n",
      "轮次 30, 损失: 3.7613\n",
      "轮次 32, 损失: 3.4649\n",
      "轮次 34, 损失: 3.1974\n",
      "轮次 36, 损失: 2.9558\n",
      "轮次 38, 损失: 2.7375\n",
      "轮次 40, 损失: 2.5400\n",
      "轮次 42, 损失: 2.3612\n",
      "轮次 44, 损失: 2.1992\n",
      "轮次 46, 损失: 2.0524\n",
      "轮次 48, 损失: 1.9192\n",
      "轮次 50, 损失: 1.7982\n",
      "轮次 52, 损失: 1.6883\n",
      "轮次 54, 损失: 1.5883\n",
      "轮次 56, 损失: 1.4974\n",
      "轮次 58, 损失: 1.4146\n",
      "轮次 60, 损失: 1.3391\n",
      "轮次 62, 损失: 1.2703\n",
      "轮次 64, 损失: 1.2074\n",
      "轮次 66, 损失: 1.1500\n",
      "轮次 68, 损失: 1.0976\n",
      "轮次 70, 损失: 1.0496\n",
      "轮次 72, 损失: 1.0056\n",
      "轮次 74, 损失: 0.9654\n",
      "轮次 76, 损失: 0.9285\n",
      "轮次 78, 损失: 0.8946\n",
      "轮次 80, 损失: 0.8635\n",
      "轮次 82, 损失: 0.8349\n",
      "轮次 84, 损失: 0.8086\n",
      "轮次 86, 损失: 0.7844\n",
      "轮次 88, 损失: 0.7621\n",
      "轮次 90, 损失: 0.7415\n",
      "轮次 92, 损失: 0.7225\n",
      "轮次 94, 损失: 0.7050\n",
      "轮次 96, 损失: 0.6888\n",
      "轮次 98, 损失: 0.6738\n",
      "轮次 100, 损失: 0.6599\n",
      "轮次 102, 损失: 0.6470\n",
      "轮次 104, 损失: 0.6351\n",
      "轮次 106, 损失: 0.6240\n",
      "轮次 108, 损失: 0.6137\n",
      "轮次 110, 损失: 0.6041\n",
      "轮次 112, 损失: 0.5952\n",
      "轮次 114, 损失: 0.5870\n",
      "轮次 116, 损失: 0.5792\n",
      "轮次 118, 损失: 0.5720\n",
      "轮次 120, 损失: 0.5653\n",
      "轮次 122, 损失: 0.5590\n",
      "轮次 124, 损失: 0.5531\n",
      "轮次 126, 损失: 0.5476\n",
      "轮次 128, 损失: 0.5424\n",
      "轮次 130, 损失: 0.5376\n",
      "轮次 132, 损失: 0.5330\n",
      "轮次 134, 损失: 0.5287\n",
      "轮次 136, 损失: 0.5247\n",
      "轮次 138, 损失: 0.5209\n",
      "轮次 140, 损失: 0.5174\n",
      "轮次 142, 损失: 0.5140\n",
      "轮次 144, 损失: 0.5109\n",
      "轮次 146, 损失: 0.5079\n",
      "轮次 148, 损失: 0.5050\n",
      "轮次 150, 损失: 0.5024\n",
      "轮次 152, 损失: 0.4998\n",
      "轮次 154, 损失: 0.4974\n",
      "轮次 156, 损失: 0.4952\n",
      "轮次 158, 损失: 0.4930\n",
      "轮次 160, 损失: 0.4910\n",
      "轮次 162, 损失: 0.4890\n",
      "轮次 164, 损失: 0.4872\n",
      "轮次 166, 损失: 0.4854\n",
      "轮次 168, 损失: 0.4837\n",
      "轮次 170, 损失: 0.4821\n",
      "轮次 172, 损失: 0.4806\n",
      "轮次 174, 损失: 0.4791\n",
      "轮次 176, 损失: 0.4777\n",
      "轮次 178, 损失: 0.4764\n",
      "轮次 180, 损失: 0.4751\n",
      "轮次 182, 损失: 0.4739\n",
      "轮次 184, 损失: 0.4727\n",
      "轮次 186, 损失: 0.4716\n",
      "轮次 188, 损失: 0.4705\n",
      "轮次 190, 损失: 0.4695\n",
      "轮次 192, 损失: 0.4684\n",
      "轮次 194, 损失: 0.4675\n",
      "轮次 196, 损失: 0.4665\n",
      "轮次 198, 损失: 0.4657\n",
      "轮次 200, 损失: 0.4648\n",
      "轮次 202, 损失: 0.4639\n",
      "轮次 204, 损失: 0.4631\n",
      "轮次 206, 损失: 0.4624\n",
      "轮次 208, 损失: 0.4616\n",
      "轮次 210, 损失: 0.4609\n",
      "轮次 212, 损失: 0.4602\n",
      "轮次 214, 损失: 0.4595\n",
      "轮次 216, 损失: 0.4588\n",
      "轮次 218, 损失: 0.4581\n",
      "轮次 220, 损失: 0.4575\n",
      "轮次 222, 损失: 0.4569\n",
      "轮次 224, 损失: 0.4563\n",
      "轮次 226, 损失: 0.4557\n",
      "轮次 228, 损失: 0.4552\n",
      "轮次 230, 损失: 0.4546\n",
      "轮次 232, 损失: 0.4541\n",
      "轮次 234, 损失: 0.4535\n",
      "轮次 236, 损失: 0.4530\n",
      "轮次 238, 损失: 0.4525\n",
      "轮次 240, 损失: 0.4520\n",
      "轮次 242, 损失: 0.4515\n",
      "轮次 244, 损失: 0.4511\n",
      "轮次 246, 损失: 0.4506\n",
      "轮次 248, 损失: 0.4501\n",
      "轮次 250, 损失: 0.4497\n",
      "轮次 252, 损失: 0.4493\n",
      "轮次 254, 损失: 0.4488\n",
      "轮次 256, 损失: 0.4484\n",
      "轮次 258, 损失: 0.4480\n",
      "轮次 260, 损失: 0.4476\n",
      "轮次 262, 损失: 0.4472\n",
      "轮次 264, 损失: 0.4468\n",
      "轮次 266, 损失: 0.4464\n",
      "轮次 268, 损失: 0.4460\n",
      "轮次 270, 损失: 0.4457\n",
      "轮次 272, 损失: 0.4453\n",
      "轮次 274, 损失: 0.4449\n",
      "轮次 276, 损失: 0.4446\n",
      "轮次 278, 损失: 0.4442\n",
      "轮次 280, 损失: 0.4439\n",
      "轮次 282, 损失: 0.4435\n",
      "轮次 284, 损失: 0.4432\n",
      "轮次 286, 损失: 0.4428\n",
      "轮次 288, 损失: 0.4425\n",
      "轮次 290, 损失: 0.4422\n",
      "轮次 292, 损失: 0.4419\n",
      "轮次 294, 损失: 0.4415\n",
      "轮次 296, 损失: 0.4412\n",
      "轮次 298, 损失: 0.4409\n",
      "轮次 300, 损失: 0.4406\n",
      "轮次 302, 损失: 0.4403\n",
      "轮次 304, 损失: 0.4400\n",
      "轮次 306, 损失: 0.4397\n",
      "轮次 308, 损失: 0.4394\n",
      "轮次 310, 损失: 0.4391\n",
      "轮次 312, 损失: 0.4388\n",
      "轮次 314, 损失: 0.4385\n",
      "轮次 316, 损失: 0.4382\n",
      "轮次 318, 损失: 0.4379\n",
      "轮次 320, 损失: 0.4377\n",
      "轮次 322, 损失: 0.4374\n",
      "轮次 324, 损失: 0.4371\n",
      "轮次 326, 损失: 0.4368\n",
      "轮次 328, 损失: 0.4366\n",
      "轮次 330, 损失: 0.4363\n",
      "轮次 332, 损失: 0.4360\n",
      "轮次 334, 损失: 0.4358\n",
      "轮次 336, 损失: 0.4355\n",
      "轮次 338, 损失: 0.4352\n",
      "轮次 340, 损失: 0.4350\n",
      "轮次 342, 损失: 0.4347\n",
      "轮次 344, 损失: 0.4345\n",
      "轮次 346, 损失: 0.4342\n",
      "轮次 348, 损失: 0.4339\n",
      "轮次 350, 损失: 0.4337\n",
      "轮次 352, 损失: 0.4334\n",
      "轮次 354, 损失: 0.4332\n",
      "轮次 356, 损失: 0.4330\n",
      "轮次 358, 损失: 0.4327\n",
      "轮次 360, 损失: 0.4325\n",
      "轮次 362, 损失: 0.4322\n",
      "轮次 364, 损失: 0.4320\n",
      "轮次 366, 损失: 0.4318\n",
      "轮次 368, 损失: 0.4315\n",
      "轮次 370, 损失: 0.4313\n",
      "轮次 372, 损失: 0.4311\n",
      "轮次 374, 损失: 0.4308\n",
      "轮次 376, 损失: 0.4306\n",
      "轮次 378, 损失: 0.4304\n",
      "轮次 380, 损失: 0.4301\n",
      "轮次 382, 损失: 0.4299\n",
      "轮次 384, 损失: 0.4297\n",
      "轮次 386, 损失: 0.4295\n",
      "轮次 388, 损失: 0.4293\n",
      "轮次 390, 损失: 0.4290\n",
      "轮次 392, 损失: 0.4288\n",
      "轮次 394, 损失: 0.4286\n",
      "轮次 396, 损失: 0.4284\n",
      "轮次 398, 损失: 0.4282\n",
      "轮次 400, 损失: 0.4280\n",
      "轮次 402, 损失: 0.4278\n",
      "轮次 404, 损失: 0.4275\n",
      "轮次 406, 损失: 0.4273\n",
      "轮次 408, 损失: 0.4271\n",
      "轮次 410, 损失: 0.4269\n",
      "轮次 412, 损失: 0.4267\n",
      "轮次 414, 损失: 0.4265\n",
      "轮次 416, 损失: 0.4263\n",
      "轮次 418, 损失: 0.4261\n",
      "轮次 420, 损失: 0.4259\n",
      "轮次 422, 损失: 0.4257\n",
      "轮次 424, 损失: 0.4255\n",
      "轮次 426, 损失: 0.4253\n",
      "轮次 428, 损失: 0.4251\n",
      "轮次 430, 损失: 0.4249\n",
      "轮次 432, 损失: 0.4247\n",
      "轮次 434, 损失: 0.4245\n",
      "轮次 436, 损失: 0.4244\n",
      "轮次 438, 损失: 0.4242\n",
      "轮次 440, 损失: 0.4240\n",
      "轮次 442, 损失: 0.4238\n",
      "轮次 444, 损失: 0.4236\n",
      "轮次 446, 损失: 0.4234\n",
      "轮次 448, 损失: 0.4232\n",
      "轮次 450, 损失: 0.4230\n",
      "轮次 452, 损失: 0.4229\n",
      "轮次 454, 损失: 0.4227\n",
      "轮次 456, 损失: 0.4225\n",
      "轮次 458, 损失: 0.4223\n",
      "轮次 460, 损失: 0.4221\n",
      "轮次 462, 损失: 0.4220\n",
      "轮次 464, 损失: 0.4218\n",
      "轮次 466, 损失: 0.4216\n",
      "轮次 468, 损失: 0.4214\n",
      "轮次 470, 损失: 0.4213\n",
      "轮次 472, 损失: 0.4211\n",
      "轮次 474, 损失: 0.4209\n",
      "轮次 476, 损失: 0.4208\n",
      "轮次 478, 损失: 0.4206\n",
      "轮次 480, 损失: 0.4204\n",
      "轮次 482, 损失: 0.4202\n",
      "轮次 484, 损失: 0.4201\n",
      "轮次 486, 损失: 0.4199\n",
      "轮次 488, 损失: 0.4198\n",
      "轮次 490, 损失: 0.4196\n",
      "轮次 492, 损失: 0.4194\n",
      "轮次 494, 损失: 0.4193\n",
      "轮次 496, 损失: 0.4191\n",
      "轮次 498, 损失: 0.4189\n",
      "轮次 500, 损失: 0.4188\n",
      "轮次 502, 损失: 0.4186\n",
      "轮次 504, 损失: 0.4185\n",
      "轮次 506, 损失: 0.4183\n",
      "轮次 508, 损失: 0.4182\n",
      "轮次 510, 损失: 0.4180\n",
      "轮次 512, 损失: 0.4178\n",
      "轮次 514, 损失: 0.4177\n",
      "轮次 516, 损失: 0.4175\n",
      "轮次 518, 损失: 0.4174\n",
      "轮次 520, 损失: 0.4172\n",
      "轮次 522, 损失: 0.4171\n",
      "轮次 524, 损失: 0.4169\n",
      "轮次 526, 损失: 0.4168\n",
      "轮次 528, 损失: 0.4166\n",
      "轮次 530, 损失: 0.4165\n",
      "轮次 532, 损失: 0.4163\n",
      "轮次 534, 损失: 0.4162\n",
      "轮次 536, 损失: 0.4161\n",
      "轮次 538, 损失: 0.4159\n",
      "轮次 540, 损失: 0.4158\n",
      "轮次 542, 损失: 0.4156\n",
      "轮次 544, 损失: 0.4155\n",
      "轮次 546, 损失: 0.4153\n",
      "轮次 548, 损失: 0.4152\n",
      "轮次 550, 损失: 0.4151\n",
      "轮次 552, 损失: 0.4149\n",
      "轮次 554, 损失: 0.4148\n",
      "轮次 556, 损失: 0.4147\n",
      "轮次 558, 损失: 0.4145\n",
      "轮次 560, 损失: 0.4144\n",
      "轮次 562, 损失: 0.4142\n",
      "轮次 564, 损失: 0.4141\n",
      "轮次 566, 损失: 0.4140\n",
      "轮次 568, 损失: 0.4138\n",
      "轮次 570, 损失: 0.4137\n",
      "轮次 572, 损失: 0.4136\n",
      "轮次 574, 损失: 0.4135\n",
      "轮次 576, 损失: 0.4133\n",
      "轮次 578, 损失: 0.4132\n",
      "轮次 580, 损失: 0.4131\n",
      "轮次 582, 损失: 0.4129\n",
      "轮次 584, 损失: 0.4128\n",
      "轮次 586, 损失: 0.4127\n",
      "轮次 588, 损失: 0.4126\n",
      "轮次 590, 损失: 0.4124\n",
      "轮次 592, 损失: 0.4123\n",
      "轮次 594, 损失: 0.4122\n",
      "轮次 596, 损失: 0.4121\n",
      "轮次 598, 损失: 0.4119\n",
      "轮次 600, 损失: 0.4118\n",
      "轮次 602, 损失: 0.4117\n",
      "轮次 604, 损失: 0.4116\n",
      "轮次 606, 损失: 0.4115\n",
      "轮次 608, 损失: 0.4113\n",
      "轮次 610, 损失: 0.4112\n",
      "轮次 612, 损失: 0.4111\n",
      "轮次 614, 损失: 0.4110\n",
      "轮次 616, 损失: 0.4109\n",
      "轮次 618, 损失: 0.4108\n",
      "轮次 620, 损失: 0.4106\n",
      "轮次 622, 损失: 0.4105\n",
      "轮次 624, 损失: 0.4104\n",
      "轮次 626, 损失: 0.4103\n",
      "轮次 628, 损失: 0.4102\n",
      "轮次 630, 损失: 0.4101\n",
      "轮次 632, 损失: 0.4100\n",
      "轮次 634, 损失: 0.4099\n",
      "轮次 636, 损失: 0.4097\n",
      "轮次 638, 损失: 0.4096\n",
      "轮次 640, 损失: 0.4095\n",
      "轮次 642, 损失: 0.4094\n",
      "轮次 644, 损失: 0.4093\n",
      "轮次 646, 损失: 0.4092\n",
      "轮次 648, 损失: 0.4091\n",
      "轮次 650, 损失: 0.4090\n",
      "轮次 652, 损失: 0.4089\n",
      "轮次 654, 损失: 0.4088\n",
      "轮次 656, 损失: 0.4087\n",
      "轮次 658, 损失: 0.4086\n",
      "轮次 660, 损失: 0.4085\n",
      "轮次 662, 损失: 0.4084\n",
      "轮次 664, 损失: 0.4083\n",
      "轮次 666, 损失: 0.4082\n",
      "轮次 668, 损失: 0.4081\n",
      "轮次 670, 损失: 0.4080\n",
      "轮次 672, 损失: 0.4079\n",
      "轮次 674, 损失: 0.4078\n",
      "轮次 676, 损失: 0.4077\n",
      "轮次 678, 损失: 0.4076\n",
      "轮次 680, 损失: 0.4075\n",
      "轮次 682, 损失: 0.4074\n",
      "轮次 684, 损失: 0.4073\n",
      "轮次 686, 损失: 0.4072\n",
      "轮次 688, 损失: 0.4071\n",
      "轮次 690, 损失: 0.4070\n",
      "轮次 692, 损失: 0.4069\n",
      "轮次 694, 损失: 0.4068\n",
      "轮次 696, 损失: 0.4067\n",
      "轮次 698, 损失: 0.4066\n",
      "轮次 700, 损失: 0.4065\n",
      "轮次 702, 损失: 0.4064\n",
      "轮次 704, 损失: 0.4063\n",
      "轮次 706, 损失: 0.4062\n",
      "轮次 708, 损失: 0.4061\n",
      "轮次 710, 损失: 0.4060\n",
      "轮次 712, 损失: 0.4060\n",
      "轮次 714, 损失: 0.4059\n",
      "轮次 716, 损失: 0.4058\n",
      "轮次 718, 损失: 0.4057\n",
      "轮次 720, 损失: 0.4056\n",
      "轮次 722, 损失: 0.4055\n",
      "轮次 724, 损失: 0.4054\n",
      "轮次 726, 损失: 0.4053\n",
      "轮次 728, 损失: 0.4052\n",
      "轮次 730, 损失: 0.4052\n",
      "轮次 732, 损失: 0.4051\n",
      "轮次 734, 损失: 0.4050\n",
      "轮次 736, 损失: 0.4049\n",
      "轮次 738, 损失: 0.4048\n",
      "轮次 740, 损失: 0.4047\n",
      "轮次 742, 损失: 0.4047\n",
      "轮次 744, 损失: 0.4046\n",
      "轮次 746, 损失: 0.4045\n",
      "轮次 748, 损失: 0.4044\n",
      "轮次 750, 损失: 0.4043\n",
      "轮次 752, 损失: 0.4042\n",
      "轮次 754, 损失: 0.4042\n",
      "轮次 756, 损失: 0.4041\n",
      "轮次 758, 损失: 0.4040\n",
      "轮次 760, 损失: 0.4039\n",
      "轮次 762, 损失: 0.4038\n",
      "轮次 764, 损失: 0.4038\n",
      "轮次 766, 损失: 0.4037\n",
      "轮次 768, 损失: 0.4036\n",
      "轮次 770, 损失: 0.4035\n",
      "轮次 772, 损失: 0.4035\n",
      "轮次 774, 损失: 0.4034\n",
      "轮次 776, 损失: 0.4033\n",
      "轮次 778, 损失: 0.4032\n",
      "轮次 780, 损失: 0.4031\n",
      "轮次 782, 损失: 0.4031\n",
      "轮次 784, 损失: 0.4030\n",
      "轮次 786, 损失: 0.4029\n",
      "轮次 788, 损失: 0.4029\n",
      "轮次 790, 损失: 0.4028\n",
      "轮次 792, 损失: 0.4027\n",
      "轮次 794, 损失: 0.4026\n",
      "轮次 796, 损失: 0.4026\n",
      "轮次 798, 损失: 0.4025\n",
      "轮次 800, 损失: 0.4024\n",
      "轮次 802, 损失: 0.4023\n",
      "轮次 804, 损失: 0.4023\n",
      "轮次 806, 损失: 0.4022\n",
      "轮次 808, 损失: 0.4021\n",
      "轮次 810, 损失: 0.4021\n",
      "轮次 812, 损失: 0.4020\n",
      "轮次 814, 损失: 0.4019\n",
      "轮次 816, 损失: 0.4019\n",
      "轮次 818, 损失: 0.4018\n",
      "轮次 820, 损失: 0.4017\n",
      "轮次 822, 损失: 0.4016\n",
      "轮次 824, 损失: 0.4016\n",
      "轮次 826, 损失: 0.4015\n",
      "轮次 828, 损失: 0.4014\n",
      "轮次 830, 损失: 0.4014\n",
      "轮次 832, 损失: 0.4013\n",
      "轮次 834, 损失: 0.4012\n",
      "轮次 836, 损失: 0.4012\n",
      "轮次 838, 损失: 0.4011\n",
      "轮次 840, 损失: 0.4011\n",
      "轮次 842, 损失: 0.4010\n",
      "轮次 844, 损失: 0.4009\n",
      "轮次 846, 损失: 0.4009\n",
      "轮次 848, 损失: 0.4008\n",
      "轮次 850, 损失: 0.4007\n",
      "轮次 852, 损失: 0.4007\n",
      "轮次 854, 损失: 0.4006\n",
      "轮次 856, 损失: 0.4005\n",
      "轮次 858, 损失: 0.4005\n",
      "轮次 860, 损失: 0.4004\n",
      "轮次 862, 损失: 0.4004\n",
      "轮次 864, 损失: 0.4003\n",
      "轮次 866, 损失: 0.4002\n",
      "轮次 868, 损失: 0.4002\n",
      "轮次 870, 损失: 0.4001\n",
      "轮次 872, 损失: 0.4001\n",
      "轮次 874, 损失: 0.4000\n",
      "轮次 876, 损失: 0.3999\n",
      "轮次 878, 损失: 0.3999\n",
      "轮次 880, 损失: 0.3998\n",
      "轮次 882, 损失: 0.3998\n",
      "轮次 884, 损失: 0.3997\n",
      "轮次 886, 损失: 0.3997\n",
      "轮次 888, 损失: 0.3996\n",
      "轮次 890, 损失: 0.3995\n",
      "轮次 892, 损失: 0.3995\n",
      "轮次 894, 损失: 0.3994\n",
      "轮次 896, 损失: 0.3994\n",
      "轮次 898, 损失: 0.3993\n",
      "轮次 900, 损失: 0.3993\n",
      "轮次 902, 损失: 0.3992\n",
      "轮次 904, 损失: 0.3991\n",
      "轮次 906, 损失: 0.3991\n",
      "轮次 908, 损失: 0.3990\n",
      "轮次 910, 损失: 0.3990\n",
      "轮次 912, 损失: 0.3989\n",
      "轮次 914, 损失: 0.3989\n",
      "轮次 916, 损失: 0.3988\n",
      "轮次 918, 损失: 0.3988\n",
      "轮次 920, 损失: 0.3987\n",
      "轮次 922, 损失: 0.3987\n",
      "轮次 924, 损失: 0.3986\n",
      "轮次 926, 损失: 0.3986\n",
      "轮次 928, 损失: 0.3985\n",
      "轮次 930, 损失: 0.3985\n",
      "轮次 932, 损失: 0.3984\n",
      "轮次 934, 损失: 0.3984\n",
      "轮次 936, 损失: 0.3983\n",
      "轮次 938, 损失: 0.3983\n",
      "轮次 940, 损失: 0.3982\n",
      "轮次 942, 损失: 0.3982\n",
      "轮次 944, 损失: 0.3981\n",
      "轮次 946, 损失: 0.3981\n",
      "轮次 948, 损失: 0.3980\n",
      "轮次 950, 损失: 0.3980\n",
      "轮次 952, 损失: 0.3979\n",
      "轮次 954, 损失: 0.3979\n",
      "轮次 956, 损失: 0.3978\n",
      "轮次 958, 损失: 0.3978\n",
      "轮次 960, 损失: 0.3977\n",
      "轮次 962, 损失: 0.3977\n",
      "轮次 964, 损失: 0.3976\n",
      "轮次 966, 损失: 0.3976\n",
      "轮次 968, 损失: 0.3975\n",
      "轮次 970, 损失: 0.3975\n",
      "轮次 972, 损失: 0.3974\n",
      "轮次 974, 损失: 0.3974\n",
      "轮次 976, 损失: 0.3973\n",
      "轮次 978, 损失: 0.3973\n",
      "轮次 980, 损失: 0.3973\n",
      "轮次 982, 损失: 0.3972\n",
      "轮次 984, 损失: 0.3972\n",
      "轮次 986, 损失: 0.3971\n",
      "轮次 988, 损失: 0.3971\n",
      "轮次 990, 损失: 0.3970\n",
      "轮次 992, 损失: 0.3970\n",
      "轮次 994, 损失: 0.3969\n",
      "轮次 996, 损失: 0.3969\n",
      "轮次 998, 损失: 0.3969\n",
      "\n",
      "训练完成！最终损失: 0.3968\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
